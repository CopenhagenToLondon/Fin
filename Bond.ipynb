{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfd40e11-f104-45b0-bf56-29aaed9dc218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Callable</th>\n",
       "      <th>Convertible</th>\n",
       "      <th>Issue date</th>\n",
       "      <th>Maturity date</th>\n",
       "      <th>Coupon size</th>\n",
       "      <th>Coupon frequency</th>\n",
       "      <th>Coupon type</th>\n",
       "      <th>Issue value</th>\n",
       "      <th>Current value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nykredit</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>21-07-2017</td>\n",
       "      <td>21-07-2027</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nykredit</td>\n",
       "      <td>NO</td>\n",
       "      <td>NO</td>\n",
       "      <td>21-07-2017</td>\n",
       "      <td>21-07-2027</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2</td>\n",
       "      <td>Fixed</td>\n",
       "      <td>100</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name Callable Convertible  Issue date Maturity date  Coupon size  \\\n",
       "0  Nykredit       NO          NO  21-07-2017    21-07-2027        0.050   \n",
       "1  Nykredit       NO          NO  21-07-2017    21-07-2027        0.035   \n",
       "\n",
       "   Coupon frequency Coupon type  Issue value  Current value  \n",
       "0                 1       Fixed          100             98  \n",
       "1                 2       Fixed          100             97  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    #[\"CompanyName\", \"Callable\", \"Convertible\", \"Issue date\", \"Maturity date\", \"Coupon size\", \"Coupon frequency\", \"Coupon type\", \"Issue value\", \"Current value\"],\n",
    "    [\"Nykredit\", \"NO\", \"NO\", \"21-07-2017\", \"21-07-2027\", 0.05, 1, \"Fixed\", 100, 98],\n",
    "    [\"Nykredit\", \"NO\", \"NO\", \"21-07-2017\", \"21-07-2027\", 0.035, 2, \"Fixed\", 100, 97]  \n",
    "]\n",
    "\n",
    "bonds = pd.DataFrame(data, columns=[\"Name\", \"Callable\", \"Convertible\", \"Issue date\", \"Maturity date\", \"Coupon size\", \"Coupon frequency\", \"Coupon type\", \"Issue value\", \"Current value\"])\n",
    "bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbfe0092-c91f-407a-adf8-142da613c885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Years from issue to maturity: 10.0\n",
      "Years from date to maturity: 4.0\n"
     ]
    }
   ],
   "source": [
    "d0 = date(int(bonds['Issue date'][0][-4:12]), int(bonds['Issue date'][0][3:5]), int(bonds['Issue date'][0][0:2]))\n",
    "dx = date.today()\n",
    "d1 = date(int(bonds['Maturity date'][0][-4:12]), int(bonds['Maturity date'][0][3:5]), int(bonds['Maturity date'][0][0:2]))\n",
    "delta = d1 - d0\n",
    "print(\"Years from issue to maturity:\",round(delta.days/365,1))\n",
    "delta2 = d1 - dx\n",
    "print(\"Years from date to maturity:\",round(delta2.days/365,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb4be3d3-249a-4746-932c-79e47fd8fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BP(data):\n",
    "    coupon = data['Coupon size'] * data['Coupon frequency']\n",
    "    dx = date.today()\n",
    "    d1 = date(int(data['Maturity date'][-4:12]), int(data['Maturity date'][3:5]), int(data['Maturity date'][0:2]))\n",
    "    delta = d1 = dx\n",
    "    maturity = round(delta2.days/365,1)\n",
    "    r = 0.05\n",
    "    facevalue = data['Issue value']\n",
    "    \n",
    "    return coupon*(1-(1+r)**(-maturity))/r + facevalue/((1+r)**maturity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f38fef6-4776-412c-b273-fbf8f31d0457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.4475450043963"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BP(bonds.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa07640-f0d0-4597-b2ce-21df44524a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. find loss distribution\n",
    "#2. calculate risk measures, (EL e.i. mean) Expected loss, (UEL e.i. var) unexpected loss, expected shortfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "97e5acc4-1b68-4a3d-9f13-38ab2e369684",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = independentBinomialLossDistribution(1,1000,0.1,100)\n",
    "b = independentPoissonLossDistribution(1,1000,0.1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f2754935-3d90-4c8c-ba2a-33b0e74520be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.3,\n",
       " 29.043243620504928,\n",
       " array([  0., 100., 100., 100.]),\n",
       " array([ 62., 100., 100., 100.]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = list([0.85,0.925,0.95,0.975])\n",
    "computeRiskMeasures(1000, a, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "13365868-eb5a-455c-a7b5-a47de0f24cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "178d672a-c428-47cf-96fa-a9c822b4d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([5.92052922e-003, 3.11606801e-002, 8.11817719e-002, 1.39575678e-001,\n",
      "       1.78142642e-001, 1.80017827e-001, 1.50014856e-001, 1.06025537e-001,\n",
      "       6.48708880e-002, 3.49012965e-002, 1.67158841e-002, 7.19822760e-003,\n",
      "       2.80983446e-003, 1.00107462e-003, 3.27419144e-004, 9.88001628e-005,\n",
      "       2.76250455e-005, 7.18422237e-006, 1.74353935e-006, 3.96039408e-007,\n",
      "       8.44189265e-008, 1.69261005e-008, 3.19895202e-009, 5.70979994e-010,\n",
      "       9.64154814e-011, 1.54264770e-011, 2.34207647e-012, 3.37843390e-013,\n",
      "       4.63582095e-014, 6.05769707e-015, 7.54555249e-016, 8.96754964e-017,\n",
      "       1.01769889e-017, 1.10372447e-018, 1.14472972e-019, 1.13612273e-020,\n",
      "       1.07964879e-021, 9.82895059e-023, 8.57650813e-024, 7.17602570e-025,\n",
      "       5.75970484e-026, 4.43622966e-027, 3.27991917e-028, 2.32846159e-029,\n",
      "       1.58758745e-030, 1.03982336e-031, 6.54351083e-033, 3.95688225e-034,\n",
      "       2.29950394e-035, 1.28436310e-036, 6.89500191e-038, 3.55779252e-039,\n",
      "       1.76449224e-040, 8.41068796e-042, 3.85284926e-043, 1.69599106e-044,\n",
      "       7.17289453e-046, 2.91419538e-047, 1.13711798e-048, 4.26038850e-050,\n",
      "       1.53224499e-051, 5.28816216e-053, 1.75074978e-054, 5.55793581e-056,\n",
      "       1.69114823e-057, 4.92966287e-059, 1.37590271e-060, 3.67483836e-062,\n",
      "       9.38619704e-064, 2.29106259e-065, 5.34007069e-067, 1.18756205e-068,\n",
      "       2.51749265e-070, 5.08217695e-072, 9.75951477e-074, 1.78068340e-075,\n",
      "       3.08290062e-077, 5.05738994e-079, 7.84885078e-081, 1.15039785e-082,\n",
      "       1.58936545e-084, 2.06545218e-086, 2.51884412e-088, 2.87502816e-090,\n",
      "       3.06237336e-092, 3.03393026e-094, 2.78512570e-096, 2.35884814e-098,\n",
      "       1.83403265e-100, 1.30150158e-102, 8.37223240e-105, 4.84223967e-107,\n",
      "       2.49314400e-109, 1.12875789e-111, 4.42402310e-114, 1.47058940e-116,\n",
      "       4.03122093e-119, 8.74925867e-122, 1.40965499e-124, 1.49883572e-127,\n",
      "       7.88860905e-131]), array([0.00592053, 0.03708121, 0.11826298, 0.25783866, 0.4359813 ,\n",
      "       0.61599913, 0.76601398, 0.87203952, 0.93691041, 0.97181171,\n",
      "       0.98852759, 0.99572582, 0.99853565, 0.99953673, 0.99986415,\n",
      "       0.99996295, 0.99999057, 0.99999776, 0.9999995 , 0.99999989,\n",
      "       0.99999998, 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        ]), array([679.21300667, 781.63982375, 837.50459712, 919.07344024]), array([ 822.63293751,  910.69301987,  958.93291328, 1035.20125665]))\n",
      "(array([6.06530660e-001, 3.03265330e-001, 7.58163325e-002, 1.26360554e-002,\n",
      "       1.57950693e-003, 1.57950693e-004, 1.31625577e-005, 9.40182694e-007,\n",
      "       5.87614184e-008, 3.26452324e-009, 1.63226162e-010, 7.41937101e-012,\n",
      "       3.09140459e-013, 1.18900176e-014, 4.24643487e-016, 1.41547829e-017,\n",
      "       4.42336966e-019, 1.30099108e-020, 3.61386410e-022, 9.51016868e-024,\n",
      "       2.37754217e-025, 5.66081469e-027, 1.28654879e-028, 2.79684520e-030,\n",
      "       5.82676084e-032, 1.16535217e-033, 2.24106186e-035, 4.15011456e-037,\n",
      "       7.41091886e-039, 1.27774463e-040, 2.12957438e-042, 3.43479739e-044,\n",
      "       5.36687093e-046, 8.13162262e-048, 1.19582686e-049, 1.70832408e-051,\n",
      "       2.37267233e-053, 3.20631396e-055, 4.21883416e-057, 5.40876175e-059,\n",
      "       6.76095218e-061, 8.24506364e-063, 9.81555195e-065, 1.14134325e-066,\n",
      "       1.29698097e-068, 1.44108996e-070, 1.56640213e-072, 1.66638525e-074,\n",
      "       1.73581797e-076, 1.77124282e-078, 1.77124282e-080, 1.73651257e-082,\n",
      "       1.66972363e-084, 1.57521097e-086, 1.45852867e-088, 1.32593516e-090,\n",
      "       1.18387068e-092, 1.03848305e-094, 8.95244009e-097, 7.58681363e-099,\n",
      "       6.32234469e-101, 5.18224975e-103, 4.17923367e-105, 3.31685212e-107,\n",
      "       2.59129072e-109, 1.99330055e-111, 1.51007618e-113, 1.12692252e-115,\n",
      "       8.28619499e-118, 6.00448912e-120, 4.28892080e-122, 3.02036676e-124,\n",
      "       2.09747692e-126, 1.43662803e-128, 9.70694613e-131, 6.47129742e-133,\n",
      "       4.25743251e-135, 2.76456657e-137, 1.77215805e-139, 1.12161902e-141,\n",
      "       7.01011889e-144, 4.32723388e-146, 2.63855724e-148, 1.58949232e-150,\n",
      "       9.46126379e-153, 5.56544929e-155, 3.23572633e-157, 1.85961283e-159,\n",
      "       1.05659820e-161, 5.93594495e-164, 3.29774719e-166, 1.81194901e-168,\n",
      "       9.84754895e-171, 5.29438116e-173, 2.81616019e-175, 1.48218957e-177,\n",
      "       7.71973736e-180, 3.97924606e-182, 2.03022758e-184, 1.02536747e-186,\n",
      "       5.12683733e-189]), array([0.60653066, 0.90979599, 0.98561232, 0.99824838, 0.99982788,\n",
      "       0.99998584, 0.999999  , 0.99999994, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       1.        ]), array([ 80.28261602, 120.05374032, 153.02816573, 186.00259115]), array([ 80.51707744, 189.49282883, 211.14689213, 243.13603991]))\n"
     ]
    }
   ],
   "source": [
    "aa = independentBinomialAnalytic(100,0.05,100,alpha)\n",
    "print(aa)\n",
    "ba = independentPoissonAnalytic(100,100,0.5,alpha)\n",
    "print(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4cbce6f6-e2e5-482c-ac4f-c85b7ecc05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#               LOSS DISTRIBUTIONS                 #\n",
    "####################################################\n",
    "\n",
    "def independentBinomialLossDistribution(N,M,p,c, fullOutput = 0):\n",
    "    \"\"\"\n",
    "    N = number of counterparties\n",
    "    M = number of simulations\n",
    "    p = probability of default\n",
    "    c = exposure\n",
    "    \"\"\"\n",
    "    U = np.random.uniform(0,1,[M,N])\n",
    "    defaultIndicator = 1*np.less(U,p)\n",
    "    lossDistribution = np.sort(np.dot(defaultIndicator,c),axis=None)\n",
    "    if fullOutput==0:\n",
    "        return lossDistribution\n",
    "    else:\n",
    "        return lossDistribution,defaultIndicator\n",
    "\n",
    "def independentPoissonLossDistribution(N,M,p,c, fullOutput = 0):\n",
    "    \"\"\"\n",
    "    N = number of counterparties\n",
    "    M = number of simulations\n",
    "    p = probability of default\n",
    "    c = exposure\n",
    "    \"\"\"\n",
    "    lam = -np.log(1-p)\n",
    "    H = np.random.poisson(lam,[M,N])\n",
    "    defaultIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(defaultIndicator,c),axis=None)\n",
    "    if fullOutput==0:\n",
    "        return lossDistribution\n",
    "    else:\n",
    "        return lossDistribution,defaultIndicator\n",
    "\n",
    "####################################################\n",
    "#               RISK MEASURES                      #\n",
    "####################################################   \n",
    "\n",
    "def expectedLoss(lossDistribution):\n",
    "    expectedLoss = np.mean(lossDistribution)\n",
    "    return expectedLoss\n",
    "\n",
    "def unExpectedLoss(lossDistribution):\n",
    "    unExpectedLoss = np.std(lossDistribution)\n",
    "    return unExpectedLoss\n",
    "    \n",
    "def computeRiskMeasures(M,lossDistribution,alpha):\n",
    "    \n",
    "    #if alpha is integer than what?\n",
    "    \"\"\"\n",
    "    M = iterations\n",
    "    alpha = confidence level\n",
    "    EL = expected loss, how much do we expect to loss given the loss dist.\n",
    "    UEL = unexpected loss, how much could we end up losing, given the loss dist.\n",
    "    ES = how much could we at max expect to loss? given loss dist.\n",
    "    var = VaR value at risk\n",
    "    \"\"\"  \n",
    "    EL = np.mean(lossDistribution)\n",
    "    UEL = np.std(lossDistribution)\n",
    "    ES = np.zeros([len(alpha)])\n",
    "    var = np.zeros([len(alpha)])\n",
    "    for j in range(0,len(alpha)):\n",
    "        ES[j] = np.mean(lossDistribution[np.ceil(alpha[j]*(M-1)).astype(int): M])        \n",
    "        var[j] = lossDistribution[np.ceil(alpha[j]*(M-1)).astype(int)]\n",
    "    return EL, UEL, var, ES     \n",
    "\n",
    "####################################################\n",
    "#               PMFs, CDFs, var, es                #\n",
    "####################################################  \n",
    "\n",
    "def independentBinomialAnalytic(N,p,c,alpha):\n",
    "    \"\"\"\n",
    "    N = number of counter parties\n",
    "    p = probability of default\n",
    "    c = exposure?\n",
    "    alpha = confidence level\n",
    "    \"\"\"\n",
    "    pmfBinomial = np.zeros(N+1)\n",
    "    for k in range(0,N+1):\n",
    "        pmfBinomial[k] =  getBC(N,k)*(p**k)*((1-p)**(N-k))\n",
    "    cdfBinomial = np.cumsum(pmfBinomial)\n",
    "    varAnalytic = c*np.interp(alpha,cdfBinomial,np.linspace(0,N,N+1))\n",
    "    esAnalytic = analyticExpectedShortfall(N,alpha,pmfBinomial,c)\n",
    "    return pmfBinomial,cdfBinomial,varAnalytic,esAnalytic\n",
    "\n",
    "def independentPoissonAnalytic(N,c,myLam,alpha):\n",
    "    \"\"\"\n",
    "    N = number of counter parties\n",
    "    p = probability of default\n",
    "    c = exposure?\n",
    "    alpha = confidence level\n",
    "    \"\"\"\n",
    "    pmfPoisson = np.zeros(N+1)\n",
    "    for k in range(0,N+1):\n",
    "        pmfPoisson[k] =  poissonDensity(myLam,k)\n",
    "    cdfPoisson = np.cumsum(pmfPoisson)\n",
    "    varAnalytic = c*np.interp(alpha,cdfPoisson,np.linspace(0,N,N+1))\n",
    "    esAnalytic = analyticExpectedShortfall(N,alpha,pmfPoisson,c)\n",
    "    return pmfPoisson,cdfPoisson,varAnalytic,esAnalytic \n",
    "\n",
    "####################################################\n",
    "#               LOSS DISTRIBUTIONS                 #\n",
    "####################################################\n",
    "\n",
    "def analyticExpectedShortfall(N,alpha,pmf,c):\n",
    "    \"\"\"\n",
    "    N = number of counter parties\n",
    "    alpha = confidence level\n",
    "    c = exposure?\n",
    "    pmf = probability mass function\n",
    "    \"\"\"\n",
    "    cdf = np.cumsum(pmf)\n",
    "    numberDefaults = np.linspace(0,N,N+1)\n",
    "    expectedShortfall = np.zeros(len(alpha))\n",
    "    for n in range(0,len(alpha)):   \n",
    "        myAlpha = np.linspace(alpha[n],1,1000)\n",
    "        nanCheck =  ~np.isnan(pmf)\n",
    "        loss = c*np.interp(myAlpha,cdf[nanCheck],numberDefaults[nanCheck])\n",
    "        prob = np.interp(loss,numberDefaults[nanCheck],pmf[nanCheck])\n",
    "        expectedShortfall[n] = np.dot(loss,prob)/np.sum(prob)\n",
    "    return expectedShortfall\n",
    "    \n",
    "####################################################\n",
    "#               LOSS DISTRIBUTIONS                 #\n",
    "####################################################\n",
    "\n",
    "def independentBinomialSimulation(N,M,p,c,alpha):\n",
    "    \"\"\"\n",
    "    N = number of counterparties\n",
    "    M = number of simulations\n",
    "    p = probability of default\n",
    "    c = exposure\n",
    "    alpha = confidence level\n",
    "    \"\"\"\n",
    "    lossDistribution = independentBinomialLossDistribution(N,M,p,c,alpha)\n",
    "    el,ul,var,es = computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es\n",
    "\n",
    "def independentPoissonSimulation(N,M,p,c,alpha):\n",
    "    \"\"\"\n",
    "    N = number of counterparties\n",
    "    M = number of simulations\n",
    "    p = probability of default\n",
    "    c = exposure\n",
    "    alpha = \n",
    "    \"\"\"\n",
    "    lossDistribution = independentPoissonLossDistribution(N,M,p,c,alpha)\n",
    "    el,ul,var,es = computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es\n",
    "\n",
    "####################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b7c6f-51c0-4fe6-9aa8-eabf93dbacc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's instantiate bonds that is defined in the Bond_Utils.py module\n",
    "\n",
    "rhea = BU.Bond(100, 0.75, 1, 2, \"Rhea\", -1)\n",
    "europa = BU.Bond(100, 1.50, 1, 2, \"Europa\", -1)\n",
    "ganymede = BU.Bond(100, 2.50, 1, 2, \"Ganymede\", -1)\n",
    "\n",
    "\n",
    "# Yield volatilities\n",
    "dy_rhea = 3.40\n",
    "dy_europa = 3.00\n",
    "dy_ganymede = 2.00\n",
    "\n",
    "\n",
    "# Rhea YTM\n",
    "coupon_rhea = 0.0075/2 * 100\n",
    "fv_rhea = 100\n",
    "price_rhea = 98.5\n",
    "\n",
    "ytm_rhea = Symbol('y')\n",
    "equation = (coupon_rhea*exp(-ytm_rhea*0.5)+\n",
    "            coupon_rhea*exp(-ytm_rhea*1.0)+\n",
    "            fv_rhea*exp(-ytm_rhea*1.0)-price_rhea)\n",
    "\n",
    "y_rhea= solve(equation, ytm_rhea)\n",
    "y_rhea[0]\n",
    "\n",
    "\n",
    "# Europa YTM\n",
    "coupon_europa = 0.015/2 * 100\n",
    "fv_europa = 100\n",
    "price_europa = 99\n",
    "\n",
    "ytm_europa = Symbol('y')\n",
    "equation = (coupon_europa*exp(-ytm_europa*0.5)+\n",
    "            coupon_europa*exp(-ytm_europa*1)+\n",
    "            coupon_europa*exp(-ytm_europa*1.5)+\n",
    "            coupon_europa*exp(-ytm_europa*2)+\n",
    "            coupon_europa*exp(-ytm_europa*2.5)+\n",
    "            fv_europa*exp(-ytm_europa*2.5)-price_europa)\n",
    "\n",
    "y_europa= solve(equation, ytm_europa)\n",
    "y_europa[0]\n",
    "\n",
    "# Ganymede YTM\n",
    "coupon_ganymede = 0.025/2 * 100\n",
    "fv_ganymede = 100\n",
    "price_ganymede = 100.5\n",
    "\n",
    "ytm_ganymede = Symbol('y')\n",
    "equation = (coupon_ganymede*exp(-ytm_ganymede*0.5)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*1)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*1.5)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*2)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*2.5)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*3.0)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*3.5)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*4.0)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*4.5)+\n",
    "            coupon_ganymede*exp(-ytm_ganymede*5.0)+\n",
    "            fv_ganymede*exp(-ytm_ganymede*5.0)-price_ganymede)\n",
    "\n",
    "y_ganymede= solve(equation, ytm_ganymede)\n",
    "y_ganymede[0]\n",
    "\n",
    "\n",
    "\n",
    "# MD for Rhea\n",
    "md_rhea = rhea.duration(float(y_rhea[0]), 'modified')\n",
    "md_rhea\n",
    "\n",
    "\n",
    "# MD for Europa\n",
    "md_europa = europa.duration(float(y_europa[0]), 'modified')\n",
    "md_europa\n",
    "\n",
    "# MD for Ganymede\n",
    "md_ganymede = ganymede.duration(float(y_ganymede[0]), 'modified')\n",
    "md_europa\n",
    "\n",
    "\n",
    "# Bond return volatility for Rhea\n",
    "md_rhea*dy_rhea\n",
    "\n",
    "\n",
    "# Bond return volatility for Europa\n",
    "md_europa*dy_europa\n",
    "\n",
    "\n",
    "# Bond return volatility for Ganymede\n",
    "md_ganymede*dy_ganymede\n",
    "\n",
    "\n",
    "# Note: we have calculated YTM for Ganymede bond in b)\n",
    "y_ganymede[0]\n",
    "\n",
    "credit_spread = 0.01\n",
    "ytm_cb = y_ganymede[0] + credit_spread\n",
    "ytm_cb\n",
    "\n",
    "\n",
    "fv_cb = 100000\n",
    "coupon_cb = 0.03/2 * fv_cb\n",
    "price_cb = coupon_cb*math.exp(-ytm_cb*0.5) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*1.0) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*1.5) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*2.0) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*2.5) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*3.0) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*3.5) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*4.0) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*4.5) +\\\n",
    "           coupon_cb*math.exp(-ytm_cb*5.0) +\\\n",
    "           fv_cb*math.exp(-ytm_cb*5.0)\n",
    "\n",
    "price_cb\n",
    "\n",
    "#fixed income derivatives\n",
    "\n",
    "spot_rates = np.array([3.005, 3.575, 3.925, 4.134, 4.412, 4.499, 4.785, 4.896, 5.001, 5.069])/100\n",
    "maturity_years = np.array([0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5])\n",
    "\n",
    "swap_rate = 0.05\n",
    "face_value = 1000000\n",
    "years_to_maturity = 5 \n",
    "\n",
    "\n",
    "fixedLegValue = [face_value * swap_rate for i in range(years_to_maturity * 2)]\n",
    "fixedLegValue[-1] += face_value\n",
    "fixedLegCashFlows = [fv * np.exp(-rate * year) for fv, year, rate in zip(fixedLegValue, maturity_years, spot_rates)]\n",
    "fixedLegValue = sum(fixedLegCashFlows)\n",
    "\n",
    "print(\"Value of the fixed leg is {:.2f}$\".format(fixedLegValue))\n",
    "\n",
    "\n",
    "flaotingLegValue = [face_value * rate for rate in spot_rates]\n",
    "flaotingLegValue[-1] = face_value\n",
    "flaotingLegCashFlows = [fv * np.exp(-rate * year) for fv, year, rate in zip(flaotingLegValue, maturity_years, spot_rates)]\n",
    "flaotingLegValue = sum(flaotingLegCashFlows)\n",
    "\n",
    "print(\"Value of the floating leg is {:.2f}$\".format(flaotingLegValue))\n",
    "\n",
    "bondPrice = fixedLegValue - flaotingLegValue\n",
    "print(\"Bond price is {:.2f}$\".format(bondPrice))\n",
    "\n",
    "X, i = sp.symbols('X i')\n",
    "spot_rates = Array(spot_rates)\n",
    "maturity_years = Array(maturity_years)\n",
    "\n",
    "eq = sp.summation((1_000_000*X)**(-spot_rates[i]*maturity_years[i]), (i, 0, 9)) + (1_000_000*(1+X))**(-spot_rates[-1]*maturity_years[-1])\n",
    "\n",
    "s = solve(eq - 1114757, X)\n",
    "\n",
    "print(\"In order for the price to be 0, the swap rate should be {:f}%\".format(s[0]*100))\n",
    "#s = 0.0383715\n",
    "\n",
    "\n",
    "spot_rates = np.array([3.005, 3.575, 3.925, 4.134, 4.412, 4.499, 4.785, 4.896, 5.001, 5.069])/100\n",
    "maturity_years = np.array([0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5])\n",
    "\n",
    "calculated_swap_rate = 0.0383715\n",
    "face_value = 1000000\n",
    "years_to_maturity = 5\n",
    "\n",
    "floatingLegValue = 1154099\n",
    "\n",
    "fixedLegValue = [face_value * calculated_swap_rate for i in range(years_to_maturity * 2)]\n",
    "fixedLegValue[-1] += face_value\n",
    "fixedLegCashFlows = [fv * np.exp(-rate * year) for fv, year, rate in zip(fixedLegValue, maturity_years, spot_rates)]\n",
    "fixedLegValue = sum(fixedLegCashFlows)\n",
    "\n",
    "bondPrice = fixedLegValue - flaotingLegValue\n",
    "\n",
    "print(bondPrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98a67d84-b9c1-4c10-a933-de33160e0004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jointDefaultProbability(p,q,myRho):\n",
    "    pr,err=nInt.quad(jointIntegrand,-5,5,args=(p,q,myRho))\n",
    "    return pr\n",
    "\n",
    "def jointIntegrand(g,p,q,myRho):\n",
    "    p1 = th.computeP(p,myRho,g)\n",
    "    p2 = th.computeP(q,myRho,g)\n",
    "    f = p1*p2*util.gaussianDensity(g,0,1)\n",
    "    return f\n",
    "\n",
    "def initializeRegion2r(N,rStart):\n",
    "    startRegion = np.zeros(N)\n",
    "    w = np.cumsum(rStart)\n",
    "    u = np.random.uniform(0,1,N)\n",
    "    for n in range(0,N):\n",
    "        if ((u[n]>0) & (u[n]<=w[0])):\n",
    "            startRegion[n] = 0\n",
    "        elif ((u[n]>w[0]) & (u[n]<1)):\n",
    "            startRegion[n] = 1\n",
    "    return startRegion    \n",
    "\n",
    "def createRatingData(K,N,T,Pin,wStart,myRho,nu,isT): \n",
    "    Q = cumulateTransitionMatrix(K,Pin)\n",
    "    if isT==1:\n",
    "        Delta = transformCumulativeTransitionMatrix_t(K,Q,nu)                            \n",
    "    else:\n",
    "        Delta = transformCumulativeTransitionMatrix(K,Q)                    \n",
    "    Y = np.zeros([N,T]) # latent variables\n",
    "    X = np.zeros([N,T]) # credit states  \n",
    "    allP = np.zeros([N,T]) # default probabilities\n",
    "    Xlast = mc.initializeCounterparties(N,wStart) # initial states\n",
    "    X0 = Xlast\n",
    "    Plast = Pin[(Xlast-1).astype(int),-1] \n",
    "    for t in range(0,T):\n",
    "        Y[:,t] = th.getY(N,1,Plast,myRho,nu,isT)    \n",
    "        for n in range(0,N):\n",
    "            if Xlast[n] == 4:\n",
    "                X[n,t] = 4\n",
    "                continue\n",
    "            else:\n",
    "                X[n,t] = migrateRating(Xlast[n],Delta,Y[n,t])\n",
    "        allP[:,t] = Pin[(Xlast-1).astype(int),-1]\n",
    "        Plast = allP[:,t]\n",
    "        Xlast = X[:,t]\n",
    "    return X,Y,Delta,allP,X0    \n",
    "    \n",
    "def simulateCorrelatedTransitionData(K,N,T,Pin,wStart,myRho): \n",
    "    Q = cumulateTransitionMatrix(K,Pin)\n",
    "    Delta = transformCumulativeTransitionMatrix(K,Q)                    \n",
    "    Y = np.zeros([N,T]) # latent variables\n",
    "    X = np.zeros([N,T]) # credit states  \n",
    "    allP = np.zeros([N,T]) # default probabilities\n",
    "    Xlast = mc.initializeCounterparties(N,wStart) # initial states\n",
    "    X0 = Xlast\n",
    "    Plast = Pin[(Xlast-1).astype(int),-1] \n",
    "    for t in range(0,T):\n",
    "        Y[:,t] = th.getY(N,1,Plast,myRho)    \n",
    "        for n in range(0,N):\n",
    "            if Xlast[n] == 4:\n",
    "                X[n,t] = 4\n",
    "                continue\n",
    "            else:\n",
    "                X[n,t] = migrateRating(Xlast[n],Delta,Y[n,t])\n",
    "        allP[:,t] = Pin[(Xlast-1).astype(int),-1]\n",
    "        Plast = allP[:,t]\n",
    "        Xlast = X[:,t]\n",
    "    return X,Y,Delta,allP,X0    \n",
    "\n",
    "\n",
    "def createRatingData2r(K,N,T,P,wStart,rStart,myRho,nu,isT): \n",
    "    Q = cumulateTransitionMatrix(K,P)\n",
    "    Delta = transformCumulativeTransitionMatrix(K,Q)\n",
    "    rId = initializeRegion2r(N,rStart).astype(int)\n",
    "    Y = np.zeros([N,T]) # latent variables\n",
    "    X = np.zeros([N,T]) # credit states\n",
    "    allP = np.zeros([N,T]) # default probabilities\n",
    "    Xlast = mc.initializeCounterparties(N,wStart) # initial states\n",
    "    X0 = Xlast\n",
    "    Plast = P[(Xlast-1).astype(int),-1] \n",
    "    for t in range(0,T):\n",
    "        Y[:,t] = th.getY2r(N,1,Plast,myRho,rId,nu,P,isT)    \n",
    "        for n in range(0,N):\n",
    "            if Xlast[n] == 4:\n",
    "                X[n,t] = 4\n",
    "                continue\n",
    "            else:\n",
    "                X[n,t] = migrateRating(Xlast[n],Delta,Y[n,t])\n",
    "        allP[:,t] = P[(Xlast-1).astype(int),-1]\n",
    "        Plast = allP[:,t]\n",
    "        Xlast = X[:,t]\n",
    "    return X,Y,Delta,allP,X0,rId           \n",
    "    \n",
    "def migrateRating(lastX,Delta,myY):\n",
    "        transitionRow = (lastX-1).astype(int)\n",
    "        myMap = Delta[transitionRow,:]    \n",
    "        if myY>=myMap[1]:\n",
    "            myX = 1\n",
    "        elif (myY<myMap[1]) & (myY>=myMap[2]):\n",
    "            myX = 2\n",
    "        elif (myY<myMap[2]) & (myY>=myMap[3]):\n",
    "            myX = 3\n",
    "        elif myY<myMap[3]:\n",
    "            myX = 4\n",
    "        return myX\n",
    "\n",
    "def cumulateTransitionMatrix(K,M):\n",
    "    H =  np.zeros([K,K]) \n",
    "    for n in range(0,K):\n",
    "        for m in range(0,K):\n",
    "            H[m,(K-1)-n] = np.sum(M[m,(K-1)-n:K])\n",
    "    return H\n",
    "\n",
    "def transformCumulativeTransitionMatrix(K,M_c):    \n",
    "    H = np.zeros([K,K])\n",
    "    for n in range(0,K):\n",
    "        for m in range(0,K):\n",
    "            if M_c[n,m]>=0.9999999:  \n",
    "                H[n,m]=5\n",
    "            elif M_c[n,m]<=0.0000001:\n",
    "                H[n,m] = -5\n",
    "            else:\n",
    "                H[n,m] = norm.ppf(M_c[n,m])\n",
    "    return H    \n",
    "\n",
    "def transformCumulativeTransitionMatrix_t(K,M_c,nu):    \n",
    "    # Element-by-element inverse-normal transform \n",
    "    # of the cumulative transition matrix\n",
    "    H = np.zeros([K,K])\n",
    "    for n in range(0,K):\n",
    "        for m in range(0,K):\n",
    "            if M_c[n,m]>=0.9999999:  \n",
    "                H[n,m] = 5\n",
    "            elif M_c[n,m]<=0.0000001:\n",
    "                H[n,m] = -5\n",
    "            else:\n",
    "                H[n,m] = myT.ppf(M_c[n,m],nu)\n",
    "    return H\n",
    "\n",
    "def getSimpleEstimationData(T,X,allP):\n",
    "    N,T = X.shape\n",
    "    kVec = np.zeros(T)\n",
    "    nVec = np.zeros(T)\n",
    "    pVec = np.zeros(T)\n",
    "    kVec[0] = np.sum(X[:,0]==4)\n",
    "    nVec[0] = N\n",
    "    pVec[0] = np.mean(allP[:,0])\n",
    "    for t in range(1,T):\n",
    "        kVec[t] = np.sum(X[:,t]==4)-np.sum(X[:,t-1]==4)\n",
    "        nVec[t] = nVec[t-1] - kVec[t-1]\n",
    "        pVec[t] = np.mean(allP[X[:,t-1]!=4,t])\n",
    "    return pVec,nVec,kVec  \n",
    "\n",
    "def get2rEstimationData(T,X,X0,rId,allP,numP):\n",
    "    N,T = X.shape\n",
    "    kMat = np.zeros([T,numP])\n",
    "    nMat = np.zeros([T,numP])\n",
    "    pMat = np.zeros([T,numP])\n",
    "    for m in range(0,numP):\n",
    "        xLoc = (rId==m).astype(bool)\n",
    "        kMat[0,m] = np.sum(X[xLoc,0]==4)\n",
    "        nMat[0,m] = np.sum(xLoc)\n",
    "        pMat[0,m] = np.mean(allP[xLoc,0])\n",
    "        for t in range(1,T):\n",
    "            kMat[t,m] = np.sum(X[xLoc,t]==4)-np.sum(X[xLoc,t-1]==4)\n",
    "            nMat[t,m] = nMat[t-1,m] - kMat[t-1,m]\n",
    "            if np.sum(xLoc)==0:\n",
    "                pMat[t,m] = 0.0                \n",
    "            else:\n",
    "                pMat[t,m] = np.mean(allP[(X[xLoc,t-1]!=4).astype(int),t])\n",
    "    return pMat,nMat,kMat     \n",
    "    \n",
    "def getCMF(g,myRho,myP,myN,myK):\n",
    "    pg = th.computeP(myP,myRho,g)\n",
    "    f=util.getBC(myN,myK)*np.power(pg,myK)*np.power(1-pg,myN-myK)\n",
    "    cmf = f*util.gaussianDensity(g,0,1)        \n",
    "    return cmf\n",
    "\n",
    "def logLSimple(x,T,pVec,nVec,kVec):\n",
    "    L = np.zeros(T)\n",
    "    for t in range(0,T):\n",
    "        L[t],err = nInt.quad(getCMF,-5,5,\n",
    "                     args=(x,pVec[t],nVec[t],kVec[t]))\n",
    "    logL = np.sum(np.log(L))\n",
    "    return -logL          \n",
    "\n",
    "def maxSimpleLogL(T,pVec,nVec,kVec):\n",
    "    myBounds = ((0.001,0.999),)                           \n",
    "    xStart = 0.5\n",
    "    r = scipy.optimize.minimize(logLSimple,\n",
    "                    xStart,args=(T,pVec,nVec,kVec), \n",
    "                    method='TNC',jac=None,bounds=myBounds,\n",
    "                    options={'maxiter':1000}) \n",
    "    return r.x,r.success    \n",
    "\n",
    "def computeSimpleScore(x0,T,pVec,nVec,kVec):\n",
    "    h = 0.00001\n",
    "    fUp = logLSimple(x0+h/2,T,pVec,nVec,kVec)\n",
    "    fDown = logLSimple(x0-h/2,T,pVec,nVec,kVec)    \n",
    "    score = np.divide(fUp-fDown,h)\n",
    "    return score\n",
    "\n",
    "def simpleFisherInformation(x0,T,pVec,nVec,kVec):\n",
    "    h = 0.000001\n",
    "    f = logLSimple(x0,T,pVec,nVec,kVec)    \n",
    "    fUp = logLSimple(x0+h,T,pVec,nVec,kVec)\n",
    "    fDown = logLSimple(x0-h,T,pVec,nVec,kVec)    \n",
    "    I = -np.divide(fUp-2*f+fDown,h**2)\n",
    "    return I\n",
    "\n",
    "def getProdCMF(g,myRho,myP,myN,myK):\n",
    "    pg = th.computeP(myP,myRho,g)\n",
    "    return np.multiply(util.getBC(myN,myK),np.power(pg,myK)*np.power(1-pg,myN-myK))\n",
    "\n",
    "def getCMF2r(g,myRho,pVec3,nVec3,kVec3):\n",
    "    myF=getProdCMF(g,myRho,pVec3,nVec3,kVec3)\n",
    "    return np.prod(myF)*util.gaussianDensity(g,0,1)\n",
    "   \n",
    "def logL2r(x,T,pMat,nMat,kMat):\n",
    "    L = np.zeros(T)\n",
    "    for t in range(0,T):\n",
    "        L[t],err = nInt.quad(getCMF2r,-5,5,args=(x,pMat[t,:],nMat[t,:],kMat[t,:]))\n",
    "    return -np.sum(np.log(L))\n",
    "\n",
    "def log2rGridSearch(gridSize,numVar,T,pMat,nMat,kMat):\n",
    "    rhoRange = np.linspace(0.001,0.999,gridSize)\n",
    "    bigF = np.zeros([gridSize,gridSize])\n",
    "    rhoGrid = np.zeros([gridSize**numVar,numVar])\n",
    "    startTime = time.time()\n",
    "    for n in range(0,gridSize):\n",
    "        for m in range(0,gridSize):\n",
    "            print(\"Running: n:%d, m: %d\" % (n+1,m+1))\n",
    "            rhoGrid = np.array([rhoRange[n],rhoRange[m]])\n",
    "            bigF[n,m] = -logL2r(rhoGrid,T,pMat,nMat,kMat)\n",
    "    print(\"Loop takes %d minutes.\" % ((time.time()-startTime)/60))\n",
    "    # Find the values associated with the biggest value of OF\n",
    "    bigMax = np.max(bigF)\n",
    "    for n in range(0,gridSize):\n",
    "        for m in range(0,gridSize):\n",
    "                if (bigF[n,m]==bigMax):\n",
    "                    myN = n\n",
    "                    myM = m\n",
    "    rhoStart = np.array([rhoRange[myN],rhoRange[myM]])\n",
    "    return rhoStart,bigF,rhoRange  \n",
    "\n",
    "def max2rLogL(T,pMat,nMat,kMat,xStart):\n",
    "    myBounds = ((0.001,0.999),(0.001,0.999))                           \n",
    "    r = scipy.optimize.minimize(logL2r,\n",
    "                    xStart,args=(T,pMat,nMat,kMat), \n",
    "                    method='TNC',jac=None,bounds=myBounds,\n",
    "                    options={'maxiter':100})    \n",
    "    return r.x,r.success    \n",
    "   \n",
    "def hessian2r(x0,epsilon,T,pMat,nMat,kMat):\n",
    "    # The first derivative\n",
    "    f1 = approx_fprime(x0,logL2r,epsilon,T,pMat,nMat,kMat) \n",
    "    n = x0.shape[0]\n",
    "    hessian = np.zeros([n,n])\n",
    "    xx = x0\n",
    "    for j in range(0,n):\n",
    "        xx0 = xx[j] # Store old value\n",
    "        xx[j] = xx0 + epsilon # Perturb with finite difference\n",
    "        # Recalculate the partial derivatives for this new point\n",
    "        f2 = approx_fprime(xx, logL2r,epsilon,T,pMat,nMat,kMat) \n",
    "        hessian[:, j] = (f2 - f1)/epsilon # scale...\n",
    "        xx[j] = xx0 # Restore initial value of x0        \n",
    "    return hessian    \n",
    "\n",
    "def score2r(x0,epsilon,T,pMat,nMat,kMat):\n",
    "    score = approx_fprime(x0,logL2r,epsilon,T,pMat,nMat,kMat) \n",
    "    return score    \n",
    "\n",
    "def mapRating(D,from_value,to_value,K):\n",
    "    if (to_value==K) & (from_value!=K):\n",
    "        d_u = D[from_value-1,to_value-1]\n",
    "        d_l = -5\n",
    "    elif (to_value==K) & (from_value==K):\n",
    "        d_u = -5\n",
    "        d_l = -5\n",
    "    else:\n",
    "        d_u = D[from_value-1,to_value]\n",
    "        d_l = D[from_value-1,to_value-1]\n",
    "    return d_l, d_u\n",
    "\n",
    "def mapRatingData(Y,D,K):\n",
    "    N,T = Y.shape\n",
    "    d_low = np.zeros([N,T-1])\n",
    "    d_upp = np.zeros([N,T-1])\n",
    "    for n in range(0,N):\n",
    "        for m in range(1,T):\n",
    "            d_low[n,m-1],d_upp[n,m-1] = mapRating(D,\n",
    "                                 Y[n,m-1].astype(int),\n",
    "                                 Y[n,m].astype(int),K)\n",
    "    return d_low,d_upp\n",
    "\n",
    "def buildCorrelationMatrix1F(x,N):\n",
    "    R = x*np.ones([N,N])+np.eye(N)\n",
    "    R[R==1+x] = 1    \n",
    "    return R\n",
    "\n",
    "def logLCopula(x,X,nu):\n",
    "    M,T = X.shape\n",
    "    R = buildCorrelationMatrix1F(x,M)\n",
    "    detR = anp.det(R)\n",
    "    Rinv = anp.inv(R)\n",
    "    V = 0\n",
    "    for t in range(0,T):\n",
    "        V += np.log(1+np.divide(np.dot(np.dot(X[:,t],Rinv),X[:,t]),nu))\n",
    "    return -(-0.5*(T*np.log(detR)+(nu+M)*V))\n",
    "\n",
    "def scoreCopula(x0,myZ,nu):\n",
    "    h = 0.000000001\n",
    "    fUp = -logLCopula(x0+h/2,myZ,nu)\n",
    "    fDown = -logLCopula(x0-h/2,myZ,nu)    \n",
    "    score = np.divide(fUp-fDown,h)\n",
    "    return score\n",
    "\n",
    "def hessianCopula(x0,myZ,nu):\n",
    "    h = 0.0001\n",
    "    f = logLCopula(x0,myZ,nu)    \n",
    "    fUp = logLCopula(x0+h,myZ,nu)\n",
    "    fDown = logLCopula(x0-h,myZ,nu)    \n",
    "    I = np.divide(fUp-2*f+fDown,h**2)\n",
    "    return I\n",
    "\n",
    "def maxCopulaLogL(startX,myZ,nu):\n",
    "    myBounds = ((0.001,0.999),)                           \n",
    "    r = scipy.optimize.minimize(logLCopula,\n",
    "                        startX,args=(myZ,nu), \n",
    "                        method='TNC',jac=None,bounds=myBounds,\n",
    "                        options={'maxiter':1000}) \n",
    "    return r.x,r.success    \n",
    "    \n",
    "\n",
    "def mixtureMethodOfMoment(x,myP,myV,myModel):\n",
    "    if myModel==0: # Beta-binomial\n",
    "        M1 = mix.betaMoment(x[0],x[1],1)\n",
    "        M2 = mix.betaMoment(x[0],x[1],2)\n",
    "    elif myModel==1: # Logit\n",
    "        M1,err = nInt.quad(mix.logitProbitMoment,-8,8,args=(x[0],x[1],1,1)) \n",
    "        M2,err = nInt.quad(mix.logitProbitMoment,-8,8,args=(x[0],x[1],2,1))\n",
    "    elif myModel==2: # Probit\n",
    "        M1,err = nInt.quad(mix.logitProbitMoment,-8,8,args=(x[0],x[1],1,0)) \n",
    "        M2,err = nInt.quad(mix.logitProbitMoment,-8,8,args=(x[0],x[1],2,0))\n",
    "    elif myModel==3: # Poisson-gamma\n",
    "        M1 = mix.poissonGammaMoment(x[0],x[1],1)\n",
    "        M2 = mix.poissonGammaMoment(x[0],x[1],2)\n",
    "    elif myModel==4: # Poisson-lognormal\n",
    "        M1,err = nInt.quad(mix.poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],1,0)) \n",
    "        M2,err = nInt.quad(mix.poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],2,0)) \n",
    "    elif myModel==5: # Poisson-Weibull\n",
    "        M1,err = nInt.quad(mix.poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],1,1)) \n",
    "        M2,err = nInt.quad(mix.poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],2,1)) \n",
    "    f1 = M1 - myP    \n",
    "    f2 =(M2-M1**2) - myV    \n",
    "    return [1e4*f1, 1e4*f2]\n",
    "        \n",
    "def integrateGaussianMoment(g,r,myP,myMoment):\n",
    "    integrand = np.power(th.computeP(myP,r,g),myMoment)\n",
    "    return integrand*util.gaussianDensity(g,0,1)\n",
    "\n",
    "def methodOfMomentsG(x,myP,myV):\n",
    "    if (x<=0) | (x>1):\n",
    "        return [100,100]\n",
    "    M1,err = nInt.quad(integrateGaussianMoment,-5,5,args=(x[0],myP,1)) \n",
    "    M2,err = nInt.quad(integrateGaussianMoment,-5,5,args=(x[0],myP,2)) \n",
    "    f1 = (M2-M1**2) - myV\n",
    "    return 1e4*f1\n",
    "\n",
    "def thresholdMoment(g,w,p1,p2,myP,whichModel,myMoment,invCdf=0):\n",
    "    d1 = util.gaussianDensity(g,0,1)\n",
    "    if whichModel==1: # t\n",
    "        d2 = util.chi2Density(w,p2)\n",
    "        integrand = np.power(th.computeP_t(myP,p1,g,w,p2),myMoment)\n",
    "    if whichModel==2: # Variance-gamma\n",
    "        d2 = util.gammaDensity(w,p2,p2)\n",
    "        integrand = np.power(th.computeP_NVM(myP,p1,g,w,p2,invCdf),myMoment)\n",
    "    if whichModel==3: # Generalized hyperbolic\n",
    "        d2 = util.gigDensity(w,p2)\n",
    "        integrand = np.power(th.computeP_NVM(myP,p1,g,w,p2,invCdf),myMoment)\n",
    "    return integrand*d1*d2\n",
    "\n",
    "def getThresholdMoments(x,myP,whichModel):\n",
    "    if whichModel==0: # Gaussian\n",
    "        M1,err = nInt.quad(integrateGaussianMoment,-5,5,args=(x[0],myP,1)) \n",
    "        M2,err = nInt.quad(integrateGaussianMoment,-5,5,args=(x[0],myP,2)) \n",
    "    elif whichModel==1: # t\n",
    "        lowerBound = np.maximum(x[1]-40,2)\n",
    "        support = [[-7,7],[lowerBound,x[1]+40]]\n",
    "        M1,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,1))\n",
    "        M2,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,2))\n",
    "    elif whichModel==2: # Variance-gamma\n",
    "        invCdf = th.nvmPpf(myP,x[1],0)\n",
    "        support = [[-7,7],[0,100]]\n",
    "        M1,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,1,invCdf))\n",
    "        M2,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,2,invCdf))\n",
    "    elif whichModel==3: # Generalized hyperbolic        \n",
    "        invCdf = th.nvmPpf(myP,x[1],1)\n",
    "        support = [[-7,7],[0,100]]\n",
    "        M1,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,1,invCdf))\n",
    "        M2,err=nInt.nquad(thresholdMoment,support,args=(x[0],x[1],myP,whichModel,2,invCdf))\n",
    "    return M1,M2\n",
    "\n",
    "def thresholdMethodOfMoment(x,myP,myV,whichModel):\n",
    "    if (x[0]<=0) | (x[0]>1):\n",
    "        return 100\n",
    "    M1,M2 = getThresholdMoments(x,myP,whichModel)\n",
    "    f1 = M1 - myP    \n",
    "    f2 =(M2-M1**2) - myV    \n",
    "    return [1e4*f1,1e4*f2]\n",
    "\n",
    "def getThresholdDefaultCorrelation(x,myP,whichModel):\n",
    "    if whichModel==0: # Gaussian\n",
    "        jp = th.jointDefaultProbability(myP,myP,x[0])\n",
    "    elif whichModel==1: # t\n",
    "        jp = th.jointDefaultProbabilityT(myP,myP,x[0],x[1])\n",
    "    elif whichModel==2: # Variance-gamma\n",
    "        jp = th.jointDefaultProbabilityNVM(myP,myP,x[0],x[1],0)\n",
    "    elif whichModel==3: # Generalized hyperbolic\n",
    "        jp = th.jointDefaultProbabilityNVM(myP,myP,x[0],x[1],1)\n",
    "    return np.divide(jp-myP**2,myP*(1-myP))\n",
    "\n",
    "def getCMF_cr(s,myW,myA,myP,myN,myK):\n",
    "    ps=myP*(1-myW)+myP*myW*s\n",
    "    f=util.getBC(myN,myK)*np.power(ps,myK)*np.power(1-ps,myN-myK)\n",
    "    return f*util.gammaDensity(s,myA,myA)        \n",
    "\n",
    "def logLSimple_cr(x,T,pVec,nVec,kVec,myA):\n",
    "    L = np.zeros(T)\n",
    "    for t in range(0,T):\n",
    "        L[t],err = nInt.quad(getCMF_cr,0,100,\n",
    "                     args=(x,myA,pVec[t],nVec[t],kVec[t]))\n",
    "    return -np.sum(np.log(L))\n",
    "\n",
    "def crPlusMoment(s,myW,myA,myP,momentNumber):\n",
    "    v0 = myP*(1-myW) + myP*myW*s\n",
    "    myDensity = util.gammaDensity(s,myA,myA)\n",
    "    return np.power(v0,momentNumber)*myDensity\n",
    "\n",
    "def crPlusScore(x0,T,pVec,nVec,kVec,myA):\n",
    "    h = 0.0000001\n",
    "    fUp = logLSimple_cr(x0+h/2,T,pVec,nVec,kVec,myA)\n",
    "    fDown = logLSimple_cr(x0-h/2,T,pVec,nVec,kVec,myA)    \n",
    "    return np.divide(fUp-fDown,h)\n",
    "\n",
    "def crPlusFisherInformation(x0,T,pVec,nVec,kVec,myA):\n",
    "    h = 0.000001\n",
    "    f = logLSimple_cr(x0,T,pVec,nVec,kVec,myA)    \n",
    "    fUp = logLSimple_cr(x0+h,T,pVec,nVec,kVec,myA)\n",
    "    fDown = logLSimple_cr(x0-h,T,pVec,nVec,kVec,myA)    \n",
    "    return -np.divide(fUp-2*f+fDown,h**2)\n",
    "\n",
    "def jointDefaultProbabilityRegion(p,q,rhoVec):\n",
    "    pr,err=nInt.quad(jointIntegrandRegion,-10,10,args=(p,q,rhoVec))\n",
    "    return pr\n",
    "\n",
    "def jointIntegrandRegion(g,p,q,rhoVec):\n",
    "    p1 = th.computeP(p,rhoVec[0],g)\n",
    "    p2 = th.computeP(q,rhoVec[1],g)\n",
    "    density = util.gaussianDensity(g,0,1)\n",
    "    f = p1*p2*density\n",
    "    return f\n",
    "\n",
    "def getRegionalDefaultCorrelation(rhoVec,myP):\n",
    "    jp = jointDefaultProbabilityRegion(myP[0],myP[1],rhoVec)\n",
    "    return np.divide(jp-myP[0]*myP[1],np.sqrt(myP[0]*(1-myP[0]))*np.sqrt(myP[1]*(1-myP[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "98dc65e5-3907-4c33-8a3c-75d6e80082db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class myPrinter(object):\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "            f.flush() \n",
    "    def flush(self) :\n",
    "        for f in self.files:\n",
    "            f.flush()    \n",
    "\n",
    "def printLine(R,myString):\n",
    "    M = len(R)\n",
    "    for m in range(0,M):\n",
    "        if m!=(M-1):\n",
    "            print(myString % (R[m]) + \" & \", end=\" \")\n",
    "        else:\n",
    "            print(myString % (R[m]) + \"\\\\\\\\\", end=\"\\n\")\n",
    "               \n",
    "def printMatrix(R,myString):\n",
    "    N,M = R.shape\n",
    "    for n in range(0,N):\n",
    "        for m in range(0,M):\n",
    "            if m!=(M-1):\n",
    "                print(myString % (R[n,m]) + \" & \", end=\" \")\n",
    "            else:\n",
    "                print(myString % (R[n,m]) + \"\\\\\\\\\", end=\"\\n\")            \n",
    "  \n",
    "\n",
    "def simulateDefaultProbabilities(N,pMean):\n",
    "    p = (pMean/1.5)*np.random.chisquare(1.5,N)\n",
    "    return p\n",
    "\n",
    "def simulateTenors(N,T):\n",
    "    tenor = np.random.uniform(0.25,T,N)\n",
    "    return tenor\n",
    "    \n",
    "def simulateRegions(N,rStart):\n",
    "    w = np.cumsum(rStart)\n",
    "    u = np.random.uniform(0,1,N)\n",
    "    myRegion = np.zeros(N)\n",
    "    for n in range(0,N):\n",
    "        if ((u[n]>=0) & (u[n]<=w[0])):\n",
    "            myRegion[n] = 1\n",
    "        elif ((u[n]>w[0]) & (u[n]<=w[1])):\n",
    "            myRegion[n] = 2\n",
    "        elif (u[n]>w[1]) & (u[n]<=1):\n",
    "            myRegion[n] = 3        \n",
    "    return myRegion    \n",
    "    \n",
    "def simulateExposures(N,portfolioSize):\n",
    "    w = np.random.weibull(1,N)\n",
    "    c = portfolioSize*np.divide(w,np.sum(w)) \n",
    "    return c\n",
    "        \n",
    "def generateGamma(a,b,N):\n",
    "    G1 = np.random.gamma(a,1,N)\n",
    "    G2 = np.random.gamma(b,1,N)\n",
    "    Z = np.divide(G1,G1+G2)  \n",
    "    return Z\n",
    "    \n",
    "def computeBeta(a,b):\n",
    "    Ga = math.gamma(a)\n",
    "    Gb = math.gamma(b)\n",
    "    Gab = math.gamma(a+b)\n",
    "    return (Ga*Gb)/Gab\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c8412-3f8e-4a6c-be3d-702c7450367e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(util)\n",
    "importlib.reload(bp)\n",
    "importlib.reload(mix)\n",
    "importlib.reload(th)\n",
    "importlib.reload(mert)\n",
    "\n",
    "def getQ(p_n,myH=-50):\n",
    "    return np.divide(1-np.exp(myH*p_n),1-np.exp(myH))\n",
    "\n",
    "def getRho(p_n,rhoMin=0.12,rhoMax=0.24):\n",
    "    myQ = getQ(p_n)\n",
    "    return rhoMin*myQ + rhoMax*(1-myQ)\n",
    "    \n",
    "def getMaturitySlope(p_n,p0=0.11582,p1=-0.05478):\n",
    "    return np.power(p0+p1*np.log(p_n),2)\n",
    "                                  \n",
    "def getMaturityAdjustment(tenor,p_n,p0=2.5):\n",
    "    myB = getMaturitySlope(p_n)\n",
    "    return np.divide(1+(tenor-p0)*myB,1-(p0-1)*myB)    \n",
    "                              \n",
    "def getBaselK(p_n,tenor,alpha):\n",
    "    g = norm.ppf(1-alpha)\n",
    "    rhoBasel = getRho(p_n)\n",
    "    ma = getMaturityAdjustment(tenor,p_n)\n",
    "    pG = th.computeP(p_n,rhoBasel,g)\n",
    "    return np.multiply(pG-p_n,ma)\n",
    "    \n",
    "def getBaselRiskCapital(p_n,tenor,c,myAlpha):\n",
    "    myCounter = len(myAlpha)    \n",
    "    riskCapital = np.zeros(myCounter)\n",
    "    for n in range(0,myCounter):\n",
    "        riskCapitalCoefficient=getBaselK(p_n,tenor,myAlpha[n])\n",
    "        riskCapital[n] = np.dot(c,riskCapitalCoefficient)\n",
    "    return riskCapital     \n",
    "    \n",
    "def runModelSuite(N,M,P,C,alpha,nu,myRho,rhoTarget,tenor,modelType):\n",
    "    startTime = time.time() \n",
    "    if modelType==0: # Binomial (independent-default) model\n",
    "        el,ul,var,es = bp.independentBinomialSimulation(N,M,P,C,alpha)\n",
    "        simTime = (time.time() - startTime)\n",
    "    elif modelType==1: # Gaussian threshold model\n",
    "        el,ul,var,es = th.oneFactorThresholdModel(N,M,P,C,myRho,nu,alpha,0)\n",
    "        simTime = (time.time() - startTime)\n",
    "    elif modelType==2: # Beta-binomial mixture model\n",
    "        myP = np.mean(P)\n",
    "        a,b = mix.betaCalibrate(myP,rhoTarget)\n",
    "        M1 = mix.betaMoment(a,b,1)\n",
    "        M2 = mix.betaMoment(a,b,2)\n",
    "        print(\"a, b parameters are %0.1f and %0.1f.\" % (a,b))\n",
    "        print(\"Targeted: %0.4f and calibrated: %0.4f default probability.\" % (myP,M1))\n",
    "        print(\"Targeted: %0.3f and calibrated: %0.3f default correlation.\" % (rhoTarget,np.divide(M2-M1**2,M1-M1**2)))\n",
    "        el,ul,var,es = mix.betaBinomialSimulation(N,M,C,a,b,alpha)\n",
    "        simTime = (time.time() - startTime)\n",
    "    elif modelType==3: # t-distributed threshold model\n",
    "        el,ul,var,es = th.oneFactorThresholdModel(N,M,P,C,myRho,nu,alpha,1)\n",
    "        simTime = (time.time() - startTime)\n",
    "    elif modelType==4: # Basel IRB approach\n",
    "        mAdjustedC = np.multiply(C,getMaturityAdjustment(tenor,P))\n",
    "        el = np.dot(P,mAdjustedC)       \n",
    "        var = getBaselRiskCapital(P,tenor,C,alpha)\n",
    "        ul = np.sum(P*(1-P)*mAdjustedC)\n",
    "        es = var\n",
    "        simTime = (time.time() - startTime)    \n",
    "    elif modelType==5: # Asymptotic Gaussian threshold model (ASRF)\n",
    "        meanP = np.maximum(0.0009,np.median(P))\n",
    "        a,b = th.getAsrfMoments(meanP,myRho)\n",
    "        el = np.sum(C)*a\n",
    "        ul = np.sum(C)*b\n",
    "        pdf,cdf,var,es = th.asrfModel(meanP,myRho,C,alpha)\n",
    "        simTime = (time.time() - startTime)    \n",
    "    return el,ul,var,es,simTime\n",
    "    \n",
    "def fG(myAlpha):\n",
    "    return norm.pdf(norm.ppf(1-myAlpha))\n",
    "    \n",
    "def dfG(myAlpha):\n",
    "    z = norm.ppf(1-myAlpha)\n",
    "    return -z*fG(myAlpha)\n",
    "  \n",
    "def mu(myAlpha,myP,myC,myRho):\n",
    "    pn = th.computeP(myP,myRho,norm.ppf(1-myAlpha))\n",
    "    return np.dot(myC,pn)\n",
    "\n",
    "def dmu(myAlpha,myP,myC,myRho):\n",
    "    constant = np.sqrt(np.divide(myRho,1-myRho))\n",
    "    ratio = norm.ppf(th.computeP(myP,myRho,norm.ppf(1-myAlpha)))\n",
    "    return -constant*np.dot(myC,norm.pdf(ratio))\n",
    "\n",
    "def d2mu(myAlpha,myP,myC,myRho):\n",
    "    constant = np.divide(myRho,1-myRho)\n",
    "    ratio = norm.ppf(th.computeP(myP,myRho,norm.ppf(1-myAlpha)))\n",
    "    return -constant*np.dot(ratio*myC,norm.pdf(ratio))\n",
    "\n",
    "def nu(myAlpha,myP,myC,myRho):\n",
    "    pn = th.computeP(myP,myRho,norm.ppf(1-myAlpha))\n",
    "    return np.dot(np.power(myC,2),pn*(1-pn))\n",
    "\n",
    "def dnu(myAlpha,myP,myC,myRho):\n",
    "    pn = th.computeP(myP,myRho,norm.ppf(1-myAlpha))\n",
    "    ratio = norm.ppf(pn)\n",
    "    constant = np.sqrt(np.divide(myRho,1-myRho))\n",
    "    return -constant*np.dot(norm.pdf(ratio)*np.power(myC,2),1-2*pn)\n",
    "\n",
    "def granularityAdjustment(myAlpha,myP,myC,myRho):\n",
    "    # Get the necessary functions and their derivatives\n",
    "    f = fG(myAlpha)    \n",
    "    df = dfG(myAlpha)            \n",
    "    dg = dmu(myAlpha,myP,myC,myRho) \n",
    "    dg2 = d2mu(myAlpha,myP,myC,myRho) \n",
    "    h = nu(myAlpha,myP,myC,myRho)    \n",
    "    dh = dnu(myAlpha,myP,myC,myRho)  \n",
    "    # Build and return granularity adjustment formula\n",
    "    t1 = np.reciprocal(dg)\n",
    "    t2 = np.divide(h*df,f)+dh\n",
    "    t3 = np.divide(h*dg2,np.power(dg,2))    \n",
    "    return -0.5*(t1*t2-t3)\n",
    "  \n",
    "def getW(myP,myA,myRho,myAlpha):\n",
    "    num = th.computeP(myP,myRho,norm.ppf(1-myAlpha))-myP\n",
    "    den = myP*(gamma.ppf(myAlpha,myA,0,1/myA)-1)\n",
    "    return np.divide(num,den)\n",
    "  \n",
    "def getC(gBar,xi):\n",
    "    gVar = xi*gBar*(1-gBar)\n",
    "    return np.divide(gBar**2+gVar,gBar)\n",
    "\n",
    "def getRK(gBar,myA,myW,myP,myAlpha):\n",
    "    q = gamma.ppf(myAlpha,myA,0,1/myA)\n",
    "    return gBar*myP*(1-myW+myW*q)\n",
    "  \n",
    "def getK(gBar,myA,myW,myP,myAlpha):\n",
    "    q = gamma.ppf(myAlpha,myA,0,1/myA)\n",
    "    return gBar*myP*myW*(q-1)\n",
    "\n",
    "def getDelta(myA,myAlpha):\n",
    "    q = gamma.ppf(myAlpha,myA,0,1/myA)\n",
    "    return (q-1)*(myA+np.divide(1-myA,q))\n",
    "\n",
    "def myLGDRatio(gBar,xi):\n",
    "    gVar = xi*gBar*(1-gBar)\n",
    "    return np.divide(gVar,gBar**2)\n",
    "\n",
    "def granularityAdjustmentCR(myA,myW,gBar,xi,p,c,myAlpha,isApprox=0):\n",
    "    myDelta = getDelta(myA,myAlpha)\n",
    "    Cn = getC(gBar,xi)\n",
    "    RKn = getRK(gBar,myA,myW,p,myAlpha)\n",
    "    Kn = getK(gBar,myA,myW,p,myAlpha)\n",
    "    KStar = np.dot(c,Kn)\n",
    "    myRatio = myLGDRatio(gBar,xi)\n",
    "    if isApprox==0:\n",
    "        t1 = myDelta*(Cn*RKn+np.power(RKn,2)*myRatio)\n",
    "        t2 = Kn*(Cn+2*RKn*myRatio)\n",
    "    else:\n",
    "        t1 = myDelta*Cn*RKn\n",
    "        t2 = Kn*Cn  \n",
    "    return np.dot(np.power(c,2),t1-t2)/(2*KStar)  \n",
    "  \n",
    "def gaContribution(myA,myW,gBar,xi,p,c,n,myAlpha,isApprox=0):\n",
    "    myDelta = getDelta(myA,myAlpha)\n",
    "    Cn = getC(gBar,xi)\n",
    "    RKn = getRK(gBar,myA,myW,p,myAlpha)\n",
    "    Kn = getK(gBar,myA,myW,p,myAlpha)\n",
    "    KStar = np.sum(c*Kn)\n",
    "    myRatio = myLGDRatio(gBar,xi)\n",
    "    if isApprox==0:\n",
    "        t1 = myDelta*(Cn*RKn+np.power(RKn,2)*myRatio)\n",
    "        t2 = Kn*(Cn+2*RKn*myRatio)\n",
    "    else:\n",
    "        t1 = myDelta*Cn*RKn\n",
    "        t2 = Kn*Cn  \n",
    "    return np.dot(np.power(c[n],2),t1[n]-t2[n])/(2*KStar)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59933f4-97ba-4346-849f-86b5ccb92704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importlib.reload(util)\n",
    "\n",
    "def inventedTransitionMatrix():\n",
    "    P = np.array([[0.960,0.029,0.010,0.001],\n",
    "              [0.100,0.775,0.120,0.005],\n",
    "              [0.120,0.220,0.650,0.010],\n",
    "              [0.000,0.000,0.000,1.00]])\n",
    "    return P\n",
    "\n",
    "def initializeCounterparties(N,wStart):\n",
    "    startRating = np.zeros(N)\n",
    "    w = np.cumsum(wStart)\n",
    "    u = np.random.uniform(0,1,N)\n",
    "    for n in range(0,N):\n",
    "        if ((u[n]>0) & (u[n]<=w[0])):\n",
    "            startRating[n] = 1\n",
    "        elif ((u[n]>w[0]) & (u[n]<=w[1])):\n",
    "            startRating[n] = 2\n",
    "        elif ((u[n]>w[1]) & (u[n]<=w[2])):\n",
    "            startRating[n] = 3        \n",
    "        elif u[n]>w[2]:\n",
    "            startRating[n] = 4\n",
    "    return startRating\n",
    "\n",
    "def simulateRatingData(N,T,P,wStart):\n",
    "    tStart = initializeCounterparties(N,wStart)\n",
    "    D = np.zeros([N,T])\n",
    "    for n in range(0,N):\n",
    "        D[n,0] = tStart[n]\n",
    "        for t in range(1,T):\n",
    "            D[n,t] = transitionStep(D[n,t-1].astype(int),P)\n",
    "    return D\n",
    "    \n",
    "def transitionStep(currentState,P):\n",
    "    myP = np.cumsum(P[currentState-1,:])\n",
    "    u = np.random.uniform(0,1)\n",
    "    if ((u>0) & (u<=myP[0])):\n",
    "        return 1\n",
    "    elif ((u>myP[0]) & (u<=myP[1])):\n",
    "        return 2\n",
    "    elif ((u>myP[1]) & (u<=myP[2])):\n",
    "        return 3        \n",
    "    elif ((u>myP[2]) & (u<=myP[3])):\n",
    "        return 4    \n",
    "    \n",
    "def getTransitionCount(K,T,N,data):\n",
    "    n_ij = np.zeros([K,K])\n",
    "    # Count the n_ij or n_{to,from} \n",
    "    # => the number of j's followed by i's.\n",
    "    for i in range(0,K): # to\n",
    "        for j in range(0,K): # from\n",
    "            for n in range(0,N): # obligor\n",
    "                for tau in range(1,T): # time\n",
    "                    if ((data[n,tau]==j+1) & (data[n,tau-1]==i+1)):\n",
    "                        #if data[n,tau]==K:\n",
    "                            # Absorbing-state (no exit)\n",
    "                        n_ij[i,j] += 1\n",
    "                            #break\n",
    "                        #else:\n",
    "                        #    n_ij[i,j] += 1   \n",
    "    #if np.sum(n_ij[-1,0])==0:\n",
    "    #    n_ij[-1,-1]=1\n",
    "    return n_ij    \n",
    "\n",
    "def estimateCohortTransitionMatrix(K,n_ij,myPeriod):\n",
    "    H = np.zeros([K,K])\n",
    "    for i in range(0,K):\n",
    "        for j in range(0,K):\n",
    "            H[i,j] = np.divide(n_ij[i,j],np.sum(n_ij[i,:]))\n",
    "    H[-1,:] = np.zeros(K)\n",
    "    H[-1,-1] = 1        \n",
    "    return anp.matrix_power(H,myPeriod)\n",
    "\n",
    "def estimateHazardRateTransitionMatrix(K,n_ij,D,myPeriod):\n",
    "    # Uses the hazard-rate technique\n",
    "    H = np.zeros([K,K])\n",
    "    # Construct the generator matrix\n",
    "    for i in range(0,K):\n",
    "        for j in range(0,K):\n",
    "            if i==j:\n",
    "                continue\n",
    "            H[i,j] = np.divide(n_ij[i,j],np.sum(D==i+1))\n",
    "    for i in range(0,K):\n",
    "        H[i,i]=-(np.sum(H[i,:]-H[i,i]))\n",
    "    M = asp.expm(myPeriod*H)\n",
    "    M[-1,:] = np.zeros(K)\n",
    "    M[-1,-1] = 1   \n",
    "    return M\n",
    "   \n",
    "def bootstrapDistribution(K,N,T,PEstimate,wStart,S):\n",
    "    PBootstrap = np.zeros([K,K,S])\n",
    "    for s in range(0,S):\n",
    "        if np.remainder(s+1,500)==0:\n",
    "            print(\"Run iteration %d\" % (s+1))\n",
    "        DBootstrap = simulateRatingData(N,T,PEstimate,wStart)\n",
    "        NBootstrap = getTransitionCount(K,T,N,DBootstrap)\n",
    "        PBootstrap[:,:,s] = estimateCohortTransitionMatrix(K,NBootstrap,1)\n",
    "    return PBootstrap\n",
    "    \n",
    "\n",
    "def estimateTransitionMatrix(K,T,N,data,myPeriod,whichModel):\n",
    "    n_ij = getTransitionCount(K,T,N,data)\n",
    "    if whichModel==0:\n",
    "        M = estimateCohortTransitionMatrix(K,n_ij,myPeriod)\n",
    "    elif whichModel==1:\n",
    "        M = estimateHazardRateTransitionMatrix(K,n_ij,data,myPeriod)\n",
    "    return M    \n",
    "\n",
    "def tpLikelihood(x,M): \n",
    "    mVector = np.reshape(M, len(M)**2)\n",
    "    L = 0\n",
    "    for i in range(0,len(x)):\n",
    "        if x[i]<=0:\n",
    "            pass\n",
    "        else:\n",
    "            L += mVector[i]*np.log(x[i])\n",
    "    return L   \n",
    "\n",
    "def hessian(x0,epsilon,M):\n",
    "    # The first derivative\n",
    "    f1 = approx_fprime(x0,tpLikelihood,epsilon,M) \n",
    "    n = x0.shape[0]\n",
    "    hessian = np.zeros([n,n])\n",
    "    xx = x0\n",
    "    for j in range(0,n):\n",
    "        xx0 = xx[j] # Store old value\n",
    "        xx[j] = xx0 + epsilon # Perturb with finite difference\n",
    "        # Recalculate the partial derivatives for this new point\n",
    "        f2 = approx_fprime(xx, tpLikelihood,epsilon,M) \n",
    "        hessian[:, j] = (f2 - f1)/epsilon # scale...\n",
    "        xx[j] = xx0 # Restore initial value of x0        \n",
    "    return hessian  \n",
    "    \n",
    "    \n",
    "def printSEConfidenceInterval(myP,se,T):\n",
    "    K = myP.shape[0]\n",
    "    coeff = myT.ppf(1-0.05/2,T-1)\n",
    "    for i in range(0,K):\n",
    "        for j in range(0,K):\n",
    "            low = np.maximum(myP[i,j]-coeff*se[i,j],0)\n",
    "            up = np.minimum(myP[i,j]+coeff*se[i,j],1)\n",
    "            if j!=(K-1):\n",
    "                print(\"[%0.2f, %0.2f]\" % (low,up) + \" & \", end=\" \")\n",
    "            else:\n",
    "                print(\"[%0.2f, %0.2f]\" % (low,up) + \"\\\\\\\\\", end=\"\\n\")\n",
    "\n",
    "def printQuantileConfidenceInterval(myP,S):\n",
    "    K = myP.shape[0]\n",
    "    for i in range(0,K):\n",
    "        for j in range(0,K):\n",
    "            sortP = np.sort(myP[i,j,:],axis=None)\n",
    "            low = sortP[np.ceil(0.025*(S-1)).astype(int)]\n",
    "            up = sortP[np.ceil(0.975*(S-1)).astype(int)]\n",
    "            if j!=(K-1):\n",
    "                print(\"[%0.2f, %0.2f]\" % (low,up) + \" & \", end=\" \")\n",
    "            else:\n",
    "                print(\"[%0.2f, %0.2f]\" % (low,up) + \"\\\\\\\\\", end=\"\\n\")\n",
    "\n",
    "def bLikelihood(N,k,pDomain):\n",
    "    L = util.getBC(N,k)*(pDomain**k)*((1-pDomain)**(N-k))\n",
    "    return L/np.abs(np.max(L))\n",
    "\n",
    "def bLogLikelihood(N,k,pDomain):\n",
    "    ell = k*np.log(pDomain) + (N-k)*np.log(1-pDomain)\n",
    "    return ell/np.abs(np.max(ell))\n",
    "    \n",
    "def bScore(N,k,pDomain):\n",
    "    s = np.divide(k,pDomain)-np.divide(N-k,1-pDomain)\n",
    "    return s\n",
    "    \n",
    "def bFisherInformation(N,k,pDomain):\n",
    "    a = np.divide(k,np.power(pDomain,2))\n",
    "    b = np.divide(N-k,np.power(1-pDomain,2))\n",
    "    I = a + b \n",
    "    return I                \n",
    "  \n",
    "def getNSSpotCurve(l,s,c,t,v=0.10):\n",
    "    den = v*t\n",
    "    level = 1\n",
    "    slope = np.divide(1-np.exp(-v*t),den)\n",
    "    curve = np.divide(1-np.exp(-v*t),den) - np.exp(-v*t)\n",
    "    return l*level + s*slope + c*curve\n",
    "\n",
    "def getStep(myT,tenor,h):\n",
    "    myRange = np.insert(tenor,0,0)\n",
    "    for n in range(0,len(tenor)):\n",
    "        if myT==0:\n",
    "            loc = 0\n",
    "        elif (myT>myRange[n]) & (myT<=myRange[n+1]):\n",
    "            loc = n\n",
    "        elif myT>myRange[n+1]:\n",
    "            loc = -1\n",
    "    try: return h[loc]\n",
    "    except:\n",
    "        print(\"No value found!\")\n",
    " \n",
    "def survivalProb(myT,tenor,h):    \n",
    "    myH = getStep(myT,tenor,h)    \n",
    "    return np.exp(-myH*myT)\n",
    "\n",
    "def defaultDensity(myT,tenor,h):\n",
    "    myH = getStep(myT,tenor,h)    \n",
    "    myS = survivalProb(myT,tenor,h)\n",
    "    return myH*myS\n",
    "\n",
    "def getCouponStream(beta,fRate,tenor,h,Delta,d):    \n",
    "    myCoupon = 0\n",
    "    for i in range(0,beta):\n",
    "        myCoupon += fRate*Delta*np.interp((i+1)*Delta,tenor,d)* \\\n",
    "                survivalProb((i+1)*Delta,tenor,h)\n",
    "    return myCoupon\n",
    "\n",
    "def getPremiumAccrual(beta,fRate,tenor,h,Delta,d):\n",
    "    myAccrual = 0\n",
    "    for i in range(0,beta):\n",
    "        sIncrement = survivalProb(i*Delta,tenor,h) - survivalProb((i+1)*Delta,tenor,h)\n",
    "        myAccrual += fRate*(Delta/2)*np.interp(i*Delta+(Delta/2),tenor,d)*sIncrement\n",
    "    return myAccrual\n",
    "\n",
    "def getProtectionPayment(beta,fRate,tenor,h,Delta,d,R):\n",
    "    myProtection = 0\n",
    "    for i in range(0,beta):\n",
    "        sIncrement = survivalProb(i*Delta,tenor,h) - survivalProb((i+1)*Delta,tenor,h)        \n",
    "        myProtection += (1-R)*np.interp(i*Delta+(Delta/2),tenor,d)*sIncrement\n",
    "    return myProtection\n",
    "\n",
    "def cdsPrice(h,cds,tenor,Delta,d,R):\n",
    "    K = len(h)\n",
    "    cStream = np.zeros(K)\n",
    "    aStream = np.zeros(K)\n",
    "    xStream = np.zeros(K)\n",
    "    beta = np.zeros(K)\n",
    "    for n in range(0,K):\n",
    "        beta = (tenor[n]/Delta).astype(int)\n",
    "        cStream[n] = getCouponStream(beta,cds[n],tenor,h,Delta,d)\n",
    "        aStream[n] = getPremiumAccrual(beta,cds[n],tenor,h,Delta,d)\n",
    "        xStream[n] = getProtectionPayment(beta,cds[n],tenor,h,Delta,d,R)\n",
    "    return cStream+aStream-xStream    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b39684-665f-4d23-8669-da6d8e359635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cmUtilities as util\n",
    "import numpy.linalg as anp\n",
    "import importlib \n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as nInt\n",
    "from scipy.stats import t as myT\n",
    "from scipy.stats import mvn\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import thresholdModels as th\n",
    "\n",
    "\n",
    "def generateCorrelationMatrix(N,rho):\n",
    "    lowerDiagonal = np.zeros([N,N])\n",
    "    for n in range(0,N):\n",
    "        for m in range(0,N):\n",
    "            if n==m:\n",
    "                lowerDiagonal[n][m] = 0.5\n",
    "            elif n>m:\n",
    "                lowerDiagonal[n][m] = rho\n",
    "            else:\n",
    "                continue                \n",
    "    C = np.transpose(lowerDiagonal)+lowerDiagonal\n",
    "    return C\n",
    "        \n",
    "def getK(mu,sigma,dt,A,myP):\n",
    "    t1 = (mu-0.5*(np.power(sigma,2)))*dt\n",
    "    t2 = np.multiply(np.multiply(norm.ppf(myP),sigma),np.sqrt(dt))\n",
    "    return np.multiply(A,np.exp(t1+t2))   \n",
    "\n",
    "def mertonIndirectSimulation(N,M,p,Omega,c,alpha):\n",
    "    Z = np.random.normal(0,1,[M,N])\n",
    "    w,v = anp.eigh(Omega)\n",
    "    H = np.dot(v,np.sqrt(np.diag(w)))\n",
    "    xi = np.dot(Z,np.transpose(H)) \n",
    "    K = norm.ppf(p)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(xi,K)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)    \n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es             \n",
    "\n",
    "def mertonDirectSimulation(N,M,K,hatA,Omega,c,alpha):\n",
    "    Z = np.random.normal(0,1,[M,N])\n",
    "    w,v = anp.eigh(Omega)\n",
    "    H = np.dot(v,np.sqrt(np.diag(w)))\n",
    "    A = np.tile(hatA,(M,1)) + np.dot(Z,np.transpose(H)) \n",
    "    lossIndicator = 1*np.less(A,K)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)    \n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es         \n",
    "\n",
    "def assignRating(rateList):\n",
    "    rv = np.random.uniform()\n",
    "    if rv>0 and rv <= 0.25: # AAA\n",
    "        myRating = rateList[0]         \n",
    "    elif rv>0.25 and rv <= 0.50: # AA\n",
    "        myRating = rateList[1] \n",
    "    elif rv>0.50 and rv <= 0.75: # A\n",
    "        myRating = rateList[2] \n",
    "    else:  # BBB\n",
    "        myRating = rateList[3] \n",
    "    return myRating \n",
    "\n",
    "def assignDebtToAssetRatio(rating):\n",
    "    if rating=='AAA': \n",
    "        debtToAssetRatio = np.random.uniform(0.40,0.45)         \n",
    "    elif rating=='AA': \n",
    "        debtToAssetRatio = np.random.uniform(0.45,0.50)         \n",
    "    elif rating=='A': \n",
    "        debtToAssetRatio = np.random.uniform(0.50,0.55)         \n",
    "    else:  # BBB\n",
    "        debtToAssetRatio = np.random.uniform(0.55,0.6)         \n",
    "    return debtToAssetRatio \n",
    "\n",
    "def assignEquityVolatility(rating):\n",
    "    if rating=='AAA': \n",
    "        equityVolatility = np.random.uniform(0.40,0.45)         \n",
    "    elif rating=='AA': \n",
    "        equityVolatility = np.random.uniform(0.45,0.50)         \n",
    "    elif rating=='A': \n",
    "        equityVolatility = np.random.uniform(0.50,0.55)         \n",
    "    else:  # BBB\n",
    "        equityVolatility = np.random.uniform(0.55,0.60)         \n",
    "    return equityVolatility     \n",
    "\n",
    "def getDelta(mu,sigma,dt,A,K):\n",
    "    t1 = np.log(K/A)\n",
    "    t2 = (mu-0.5*(sigma**2))*dt\n",
    "    return np.divide(t1-t2,sigma*np.sqrt(dt))\n",
    "\n",
    "def getD1(r,sigma,dt,A,K):\n",
    "    t1 = math.log(A/K)\n",
    "    t2 = (r+0.5*(sigma**2))*dt\n",
    "    t3 = sigma*np.sqrt(dt)\n",
    "    return np.divide(t1+t2,t3)   \n",
    "\n",
    "def getE(r,sigma,dt,A,K):\n",
    "    d1 = getD1(r,sigma,dt,A,K)\n",
    "    d2 = d1 - sigma*math.sqrt(dt)\n",
    "    return A*norm.cdf(d1)-np.exp(-r*dt)*K*norm.cdf(d2)\n",
    "\n",
    "def getOptionDelta(r,sigma,dt,A,K):\n",
    "    d1 = getD1(r,sigma,dt,A,K)\n",
    "    optionDelta = norm.cdf(d1)\n",
    "    return optionDelta \n",
    "\n",
    "def minimizeG(x,r,sigmaE,dt,E,K):\n",
    "    A = x[0]\n",
    "    if A<=0:\n",
    "        return np.inf\n",
    "    else:\n",
    "        sigmaA = x[1]\n",
    "        G1 = getE(r,sigmaA,dt,A,K)-E\n",
    "        G2 = A*getOptionDelta(r,sigmaA,dt,A,K)*(sigmaA/sigmaE)-E\n",
    "        return G1**2 + G2**2\n",
    "    \n",
    "def getVarA(mu,sigma,dt,A,K):\n",
    "    t1 = np.exp((sigma**2)*dt)\n",
    "    t2 = (A**2)*np.exp(2*mu*dt)\n",
    "    return t2*(t1-1)\n",
    "\n",
    "def getExpA(mu,dt,A):\n",
    "    return np.multiply(A,np.exp(mu*dt))\n",
    "\n",
    "def getCovAB(A,B,muA,muB,sigmaA,sigmaB,rhoAB,dt):\n",
    "    t1 = (muA+muB)*dt\n",
    "    t2 = sigmaA*sigmaB*rhoAB*dt    \n",
    "    return A*B*np.exp(t1)*(np.exp(t2)-1)\n",
    "\n",
    "def getCorAB(sigmaA,sigmaB,rhoAB,dt):\n",
    "    num = np.exp(rhoAB*sigmaA*sigmaB*dt)-1\n",
    "    tA = np.sqrt(np.exp((sigmaA**2)*dt)-1)\n",
    "    tB = np.sqrt(np.exp((sigmaB**2)*dt)-1)\n",
    "    return np.divide(num,tA*tB)   \n",
    "\n",
    "def getDefaultCorAB(A,B,muA,muB,sigmaA,sigmaB,rhoAB,dt,KA,KB):\n",
    "    dA = getDelta(muA,sigmaA,dt,A,KA)\n",
    "    dB = getDelta(muB,sigmaB,dt,B,KB)     \n",
    "    pA = norm.cdf(dA)\n",
    "    pB = norm.cdf(dB)   \n",
    "    pAB,err = mvn.mvnun(np.array([-100, -100]),np.array([dA, dB]),\n",
    "                      np.array([0, 0]),np.array([[1,rhoAB],[rhoAB,1]]))   \n",
    "    return np.divide(pAB-pA*pB,np.sqrt(pA*pB*(1-pA)*(1-pB)))\n",
    "\n",
    "\n",
    "def getSigmaA(mu,sigma,dt,A,K):\n",
    "    t1 = math.exp((sigma**2)*dt)\n",
    "    t2 = A*math.exp(mu*dt)\n",
    "    return t2*np.sqrt(t1-1)\n",
    "\n",
    "def computeAssetValueMoments(N,A,mu,sigma,C,dt):\n",
    "    hatA = np.zeros(N)\n",
    "    Omega = np.zeros([N,N])\n",
    "    for n in range(0,N):\n",
    "        hatA[n] = getExpA(mu[n],dt,A[n])\n",
    "        for m in range(0,N):\n",
    "            Omega[n,m] = getCovAB(A[n],A[m],mu[n],mu[m],\n",
    "                                  sigma[n],sigma[m],C[n,m],dt)\n",
    "    return hatA,Omega\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defa5f3-5eae-4431-b212-3f10d3f73dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cmUtilities as util\n",
    "import numpy.linalg as anp\n",
    "import importlib \n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as nInt\n",
    "import scipy\n",
    "\n",
    "importlib.reload(util)\n",
    "\n",
    "# -----------------------\n",
    "# Beta-binomial model functions\n",
    "# -----------------------\n",
    "\n",
    "def betaMoment(a,b,momentNumber):\n",
    "    num = math.gamma(a+momentNumber)*math.gamma(a+b)\n",
    "    den = math.gamma(a+b+momentNumber)*math.gamma(a)\n",
    "    f = np.divide(num,den)\n",
    "    return f\n",
    "\n",
    "def betaCalibrate(pTarget,rhoTarget):\n",
    "    A = np.array([[rhoTarget,rhoTarget],[pTarget-1,pTarget]])\n",
    "    a,b = np.dot(anp.inv(A),np.array([1-rhoTarget,0]))\n",
    "    return a,b\n",
    "\n",
    "def betaBinomialAnalytic(N,c,a,b,alpha):\n",
    "    pmfBeta = np.zeros(N+1)\n",
    "    den = util.computeBeta(a,b)\n",
    "    for k in range(0,N+1):\n",
    "        pmfBeta[k] =  util.getBC(N,k)*util.computeBeta(a+k,b+N-k)/den  \n",
    "    cdfBeta = np.cumsum(pmfBeta)\n",
    "    varAnalytic = c*np.interp(alpha,cdfBeta,np.linspace(0,N,N+1))\n",
    "    esAnalytic = util.analyticExpectedShortfall(N,alpha,pmfBeta,c)\n",
    "    return pmfBeta,cdfBeta,varAnalytic,esAnalytic\n",
    "    \n",
    "def betaBinomialSimulation(N,M,c,a,b,alpha):\n",
    "    Z = util.generateGamma(a,b,M)\n",
    "    pZ = np.transpose(np.tile(Z,(N,1)))\n",
    "    U = np.random.uniform(0,1,[M,N])\n",
    "    lossIndicator = 1*np.less(U,pZ)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es  \n",
    "\n",
    "def betaLossDistribution(N,M,c,a,b,alpha):\n",
    "    Z = util.generateGamma(a,b,M)\n",
    "    pZ = np.transpose(np.tile(Z,(N,1)))\n",
    "    U = np.random.uniform(0,1,[M,N])\n",
    "    lossIndicator = 1*np.less(U,pZ)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    return lossDistribution  \n",
    "\n",
    "# -----------------------\n",
    "# Logit-Probit model functions\n",
    "# -----------------------\n",
    "\n",
    "def logitProbitMoment(z,mu,sigma,momentNumber,isLogit):\n",
    "    if isLogit==1:\n",
    "        v0 = (np.reciprocal(1+np.exp(-(mu+sigma*z))))**momentNumber\n",
    "    else:\n",
    "        v0 = (norm.cdf(mu+sigma*z))**momentNumber    \n",
    "    density = util.gaussianDensity(z,0,1)\n",
    "    f = v0*density\n",
    "    return f  \n",
    "\n",
    "def logitProbitCalibrate(x,pTarget,rhoTarget,isLogit):\n",
    "    if x[1]<=0:\n",
    "        return [100, 100]\n",
    "    M1,err = nInt.quad(logitProbitMoment,-8,8,args=(x[0],x[1],1,isLogit)) \n",
    "    M2,err = nInt.quad(logitProbitMoment,-8,8,args=(x[0],x[1],2,isLogit)) \n",
    "    f1 = pTarget - M1    \n",
    "    f2 = rhoTarget*(M1 - (M1**2)) - (M2 - (M1**2))    \n",
    "    return [f1, f2]\n",
    "\n",
    "def logitProbitBinomialAnalytic(N,c,mu,sigma,alpha,isLogit):\n",
    "    pmf = np.zeros(N+1)\n",
    "    for k in range(0,N+1):\n",
    "        pmf[k],err=nInt.quad(logitProbitMixtureFunction,0,1,\n",
    "                                          args=(N,k,mu,sigma,isLogit)) \n",
    "    cdf = np.cumsum(pmf)\n",
    "    varAnalytic = c*np.interp(alpha,cdf,np.linspace(0,N,N+1))\n",
    "    esAnalytic = util.analyticExpectedShortfall(N,alpha,pmf,c)\n",
    "    return pmf,cdf,varAnalytic,esAnalytic\n",
    "\n",
    "def logitProbitMixtureFunction(z,N,k,mu,sigma,isLogit):\n",
    "    if isLogit==1:\n",
    "        density = util.logitDensity(z,mu,sigma)\n",
    "    else:\n",
    "        density = util.probitDensity(z,mu,sigma)\n",
    "    probTerm = util.getBC(N,k)*(z**k)*((1-z)**(N-k))\n",
    "    f = probTerm*density\n",
    "    return f    \n",
    "\n",
    "def logitProbitBinomialSimulation(N,M,c,mu,sigma,alpha,isLogit):\n",
    "    Z = np.random.normal(0,1,M)\n",
    "    if isLogit==1:\n",
    "        p = np.reciprocal(1+np.exp(-(mu+sigma*Z)))\n",
    "    else:\n",
    "        p = norm.cdf(mu+sigma*Z)\n",
    "    pZ = np.transpose(np.tile(p,(N,1)))\n",
    "    U = np.random.uniform(0,1,[M,N])\n",
    "    lossIndicator = 1*np.less(U,pZ)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es    \n",
    "\n",
    "# -----------------------\n",
    "# Poisson-Gamma model functions\n",
    "# -----------------------\n",
    "\n",
    "def poissonGammaMoment(a,b,momentNumber):\n",
    "    if momentNumber==1:\n",
    "        myMoment = 1-np.power(np.divide(b,b+1),a)\n",
    "    if momentNumber==2:\n",
    "       v1 = np.power(np.divide(b,b+1),a)\n",
    "       v2 = np.power(np.divide(b,b+2),a)\n",
    "       myMoment = 1 - 2*v1 + v2\n",
    "    return myMoment \n",
    "\n",
    "def poissonGammaCalibrate(x,pTarget,rhoTarget):\n",
    "    if x[1]<=0:\n",
    "        return [100, 100]\n",
    "    M1 = poissonGammaMoment(x[0],x[1],1)\n",
    "    M2 = poissonGammaMoment(x[0],x[1],2) \n",
    "    f1 = pTarget - M1    \n",
    "    f2 = rhoTarget*(M1 - (M1**2)) - (M2 - (M1**2))    \n",
    "    return [f1, f2]\n",
    "\n",
    "def poissonGammaAnalytic(N,c,a,b,alpha):\n",
    "    pmfPoisson = np.zeros(N+1)\n",
    "    q = np.divide(b,b+1)\n",
    "    den = math.gamma(a)\n",
    "    for k in range(0,N+1):\n",
    "        num = np.divide(math.gamma(a+k),scipy.misc.factorial(k))\n",
    "        pmfPoisson[k] = np.divide(num,den)*np.power(q,a)*np.power(1-q,k)  \n",
    "    cdfPoisson = np.cumsum(pmfPoisson)\n",
    "    varAnalytic = c*np.interp(alpha,cdfPoisson,np.linspace(0,N,N+1))\n",
    "    esAnalytic = util.analyticExpectedShortfall(N,alpha,pmfPoisson,c)\n",
    "    return pmfPoisson,cdfPoisson,varAnalytic,esAnalytic\n",
    "\n",
    "def poissonGammaSimulation(N,M,c,a,b,alpha):\n",
    "    lam = np.random.gamma(a,1/b,M)\n",
    "    H = np.zeros([M,N])\n",
    "    for m in range(0,M):\n",
    "        H[m,:] = np.random.poisson(lam[m],[N])\n",
    "    lossIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es  \n",
    " \n",
    "# -----------------------\n",
    "# Poisson-Mixture model functions\n",
    "# -----------------------\n",
    "\n",
    "def poissonMixtureMoment(s,a,b,momentNumber,whichModel):\n",
    "    v0 = 1-np.exp(-s)\n",
    "    if whichModel==0:\n",
    "        myDensity = util.logNormalDensity(s,a,b)\n",
    "    else:\n",
    "        myDensity = util.weibullDensity(s,a,b)\n",
    "    f = np.power(v0,momentNumber)*myDensity\n",
    "    return f  \n",
    "\n",
    "def poissonTransformedMixtureMoment(s,a,b,momentNumber,whichModel):\n",
    "    vy = -np.log(1-s)\n",
    "    jacobian = np.divide(1,1-s)\n",
    "    if whichModel==0:\n",
    "        psDensity = util.logNormalDensity(vy,a,b)*jacobian\n",
    "    elif whichModel==1:\n",
    "        psDensity = util.weibullDensity(vy,a,b)*jacobian\n",
    "    myMoment = (s**momentNumber)*psDensity\n",
    "    return myMoment  \n",
    "      \n",
    "def poissonMixtureAnalytic(N,myC,a,b,alpha,whichModel):\n",
    "    pmfMixture = np.zeros(N+1)\n",
    "    for k in range(0,N+1):\n",
    "        pmfMixture[k],err = nInt.quad(poissonMixtureIntegral,0,k+1,\n",
    "                                      args=(k,a,b,N,whichModel)) \n",
    "    cdfMixture = np.cumsum(pmfMixture)\n",
    "    varAnalytic = myC*np.interp(alpha,cdfMixture,np.linspace(0,N,N+1))\n",
    "    esAnalytic = util.analyticExpectedShortfall(N,alpha,pmfMixture,myC)\n",
    "    return pmfMixture,cdfMixture,varAnalytic,esAnalytic\n",
    "\n",
    "def poissonMixtureIntegral(s,k,a,b,N,whichModel):\n",
    "    pDensity = util.poissonDensity(N*s,k)\n",
    "    if whichModel==0:\n",
    "        mixDensity = util.logNormalDensity(s,a,b)\n",
    "    elif whichModel==1:\n",
    "        mixDensity = util.weibullDensity(s,a,b)        \n",
    "    f = pDensity*mixDensity\n",
    "    return f\n",
    "\n",
    "def poissonMixtureCalibrate(x,pTarget,rhoTarget,whichModel):\n",
    "    if x[1]<=0:\n",
    "        return [100, 100]\n",
    "    M1,err = nInt.quad(poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],1,whichModel)) \n",
    "    M2,err = nInt.quad(poissonMixtureMoment,0.0001,0.9999,args=(x[0],x[1],2,whichModel)) \n",
    "    f1 = pTarget - M1    \n",
    "    f2 = rhoTarget*(M1 - (M1**2)) - (M2 - (M1**2))    \n",
    "    return [f1, f2]\n",
    "\n",
    "def poissonMixtureCalibrate1(x,pTarget,rhoTarget,whichModel):\n",
    "    if x[1]<=0:\n",
    "        return [100, 100]\n",
    "    M1,err = nInt.quad(poissonTransformedMixtureMoment,0.0001,0.9999,args=(x[0],x[1],1,whichModel)) \n",
    "    M2,err = nInt.quad(poissonTransformedMixtureMoment,0.0001,0.9999,args=(x[0],x[1],2,whichModel)) \n",
    "    f1 = pTarget - M1    \n",
    "    f2 = rhoTarget*(M1 - (M1**2)) - (M2 - (M1**2))    \n",
    "    return [f1, f2]\n",
    "\n",
    "def poissonMixtureSimulation(N,M,c,a,b,alpha,whichModel):\n",
    "    if whichModel==0:\n",
    "        lam = np.random.lognormal(a,b,M)\n",
    "    elif whichModel==1:\n",
    "         lam = b*np.random.weibull(a,M)\n",
    "    H = np.zeros([M,N])\n",
    "    for m in range(0,M):\n",
    "        H[m,:] = np.random.poisson(lam[m],[N])\n",
    "    lossIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es  \n",
    "      \n",
    "# -----------------------\n",
    "# CreditRisk+ model functions\n",
    "# -----------------------\n",
    "\n",
    "def crPlusOneFactor(N,M,w,p,c,v,alpha):\n",
    "    S = np.random.gamma(v, 1/v, [M]) \n",
    "    wS =  np.transpose(np.tile(1-w + w*S,[N,1]))\n",
    "    pS = np.tile(p,[M,1])*wS\n",
    "    H = np.random.poisson(pS,[M,N])\n",
    "    lossIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es          \n",
    "\n",
    "def calibrateBeta(myG,myXi):\n",
    "    a1 = np.divide(myG*(1-myXi),myXi)\n",
    "    b1 = np.divide((1-myG)*(1-myXi),myXi)\n",
    "    return a1, b1\n",
    "\n",
    "def crPlusOneFactorLGD(N,M,w,p,c,v,gBar,xi,alpha):\n",
    "    a1,b1 = calibrateBeta(gBar,xi)\n",
    "    LGD = np.random.beta(a1,b1,[M,N])\n",
    "    S = np.random.gamma(v, 1/v, [M]) \n",
    "    wS =  np.transpose(np.tile(1-w + w*S,[N,1]))\n",
    "    pS = np.tile(p,[M,1])*wS\n",
    "    H = np.random.poisson(pS,[M,N])\n",
    "    lossIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(LGD*lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es   \n",
    "\n",
    "def crPlusMultifactor(N,M,wMat,p,c,aVec,alpha,rId):\n",
    "    K = len(aVec)\n",
    "    S = np.zeros([M,K])\n",
    "    for k in range(0,K):        \n",
    "        S[:,k] = np.random.gamma(aVec[k], 1/aVec[k], [M]) \n",
    "    W = wMat[rId,:]\n",
    "    # Could replace tile with np.kron(W[:,0],np.ones([1,M])), but it's slow\n",
    "    wS =  np.tile(W[:,0],[M,1]) + np.dot(S,np.transpose(W[:,1:]))\n",
    "    pS = np.tile(p,[M,1])*wS\n",
    "    H = np.random.poisson(pS,[M,N])\n",
    "    lossIndicator = 1*np.greater_equal(H,1)\n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es          \n",
    "      \n",
    "# -----------------------\n",
    "# Miscellaneous functions\n",
    "# -----------------------\n",
    "\n",
    "def calibrateVerify(a,b,targetP,targetRho,model):\n",
    "    if (model==0) | (model==4): # Binomial and Poisson\n",
    "        M1 = targetP\n",
    "        M2 = targetP**2\n",
    "    if model==1: # Beta-binomial\n",
    "        M1 = betaMoment(a,b,1)\n",
    "        M2 = betaMoment(a,b,2)\n",
    "    elif model==2: # Logit\n",
    "        M1,err = nInt.quad(logitProbitMoment,-8,8,args=(a,b,1,1)) \n",
    "        M2,err = nInt.quad(logitProbitMoment,-8,8,args=(a,b,2,1)) \n",
    "    elif model==3: # Probit\n",
    "        M1,err = nInt.quad(logitProbitMoment,-8,8,args=(a,b,1,0)) \n",
    "        M2,err = nInt.quad(logitProbitMoment,-8,8,args=(a,b,2,0)) \n",
    "    elif model==5: # Basic Poisson-gamma\n",
    "        M1 = poissonGammaMoment(a,b,1)\n",
    "        M2 = poissonGammaMoment(a,b,2)\n",
    "    elif model==6: # Poisson log-normal\n",
    "        M1,err = nInt.quad(poissonMixtureMoment,0,1,args=(a,b,1,0)) \n",
    "        M2,err = nInt.quad(poissonMixtureMoment,0,1,args=(a,b,2,0)) \n",
    "    elif model==7: # Poisson Weibull\n",
    "        M1,err = nInt.quad(poissonMixtureMoment,0,1,args=(a,b,1,1)) \n",
    "        M2,err = nInt.quad(poissonMixtureMoment,0,1,args=(a,b,2,1))         \n",
    "    print(\"Target default probability is %0.3f and target default correlation is %0.3f\" % (targetP,targetRho))\n",
    "    print(\"Calibrated default probability is %0.3f and calibrated default correlation is %0.3f\" % (M1,defCorr(M1,M2)))\n",
    "    return M1,M2\n",
    "\n",
    "def defCorr(M1,M2):\n",
    "    return np.divide(M2-M1**2,M1-M1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95f87c-a018-449d-990c-d0dba4d256a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cmUtilities as util\n",
    "import assetCorrelation as ac\n",
    "import importlib \n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as nInt\n",
    "from scipy.stats import t as myT\n",
    "import numpy.linalg as anp\n",
    "import scipy\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "gig = importr('GIGrvg')\n",
    "\n",
    "importlib.reload(util)\n",
    "\n",
    "def getY(N,M,p,rho,nu,isT):\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    if isT==1:\n",
    "        W = np.transpose(np.sqrt(nu/np.tile(np.random.chisquare(nu,M),(N,1))))\n",
    "        Y = np.multiply(W,math.sqrt(rho)*G + math.sqrt(1-rho)*e)\n",
    "    else:\n",
    "        Y = math.sqrt(rho)*G + math.sqrt(1-rho)*e\n",
    "    return Y   \n",
    "\n",
    "def getGaussianY(N,M,p,rho):\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    Y = math.sqrt(rho)*G + math.sqrt(1-rho)*e\n",
    "    return Y   \n",
    "\n",
    "def getTY(N,M,p,rho,nu):\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    W = np.transpose(np.sqrt(nu/np.tile(np.random.chisquare(nu,M),(N,1))))\n",
    "    Y = np.multiply(W,math.sqrt(rho)*G + math.sqrt(1-rho)*e)\n",
    "    return Y   \n",
    "\n",
    "def calibrateGaussian(x,myP,targetRho):\n",
    "    jointDefaultProb = ac.jointDefaultProbability(myP,myP,x)\n",
    "    defaultCorrelation = np.divide(jointDefaultProb-myP**2,myP*(1-myP))\n",
    "    return np.abs(defaultCorrelation-targetRho)\n",
    "\n",
    "def getY2r(N,M,p,myRho,rId,nu,P,isT):\n",
    "    rhoVector = myRho[rId]\n",
    "    rhoMatrix = np.tile(rhoVector,(M,1))\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    systematic = np.multiply(np.sqrt(rhoMatrix),G)\n",
    "    idiosyncratic = np.multiply(np.sqrt(1-rhoMatrix),e)\n",
    "    if isT==1:\n",
    "        W = np.transpose(np.sqrt(nu/np.tile(np.random.chisquare(nu,M),(N,1))))\n",
    "        Y = np.multiply(W,systematic + idiosyncratic)\n",
    "    else:\n",
    "        Y = systematic + idiosyncratic\n",
    "    return Y    \n",
    "\n",
    "def oneFactorGaussianModel(N,M,p,c,rho,alpha):\n",
    "    Y = getGaussianY(N,M,p,rho)\n",
    "    K = norm.ppf(p)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es      \n",
    "\n",
    "def oneFactorTModel(N,M,p,c,rho,nu,alpha):\n",
    "    Y = getTY(N,M,p,rho,nu)\n",
    "    K = myT.ppf(p,nu)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es      \n",
    "\n",
    "def oneFactorThresholdModel(N,M,p,c,rho,nu,alpha,isT):\n",
    "    Y = getY(N,M,p,rho,nu,isT)\n",
    "    if isT==1:\n",
    "        K = myT.ppf(p,nu)*np.ones((M,1))        \n",
    "    else:\n",
    "        K = norm.ppf(p)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es      \n",
    "\n",
    "def oneFactorThresholdLossDistribution(N,M,p,c,rho,nu,alpha,isT):\n",
    "    Y = getY(N,M,p,rho,nu,isT)\n",
    "    if isT==1:\n",
    "        K = myT.ppf(p,nu)*np.ones((M,1))        \n",
    "    else:\n",
    "        K = norm.ppf(p)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    return lossDistribution      \n",
    "    \n",
    "def asrfModel(myP,rho,c,alpha):\n",
    "    myX = np.linspace(0.0001,0.9999,100)\n",
    "    num = np.sqrt(1-rho)*norm.ppf(myX)-norm.ppf(myP)\n",
    "    cdf = norm.cdf(num/np.sqrt(rho))\n",
    "    pdf = util.asrfDensity(myX,myP,rho)\n",
    "    varAnalytic = np.sum(c)*np.interp(alpha,cdf,myX)\n",
    "    esAnalytic = asrfExpectedShortfall(alpha,myX,cdf,pdf,c,rho,myP)\n",
    "    return pdf,cdf,varAnalytic,esAnalytic\n",
    "    \n",
    "def asrfExpectedShortfall(alpha,myX,cdf,pdf,c,rho,myP):\n",
    "    expectedShortfall = np.zeros(len(alpha))\n",
    "    for n in range(0,len(alpha)):   \n",
    "        myAlpha = np.linspace(alpha[n],1,1000)\n",
    "        loss = np.sum(c)*np.interp(myAlpha,cdf,myX)\n",
    "        prob = np.interp(loss,myX,pdf)\n",
    "        expectedShortfall[n] = np.dot(loss,prob)/np.sum(prob)\n",
    "    return expectedShortfall    \n",
    "        \n",
    "def asrfMoment(x,p,rho,whichMoment):\n",
    "    if whichMoment==1:    \n",
    "        f = x*util.asrfDensity(x,p,rho)\n",
    "    elif whichMoment==2:\n",
    "        f = np.power(x,2)*util.asrfDensity(x,p,rho)\n",
    "    return f\n",
    "    \n",
    "def getAsrfMoments(p,rho):\n",
    "    el,err = nInt.quad(asrfMoment,0,1,args=(p,rho,1)) \n",
    "    M2,err = nInt.quad(asrfMoment,0,1,args=(p,rho,2)) \n",
    "    ul = np.sqrt(M2 - np.power(el,2))\n",
    "    return el, ul    \n",
    "\n",
    "def computeP_t(p,rho,y,w,nu):\n",
    "    num = np.sqrt(w/nu)*myT.ppf(p,nu)-np.multiply(np.sqrt(rho),y)\n",
    "    pZ = norm.cdf(np.divide(num,np.sqrt(1-rho)))\n",
    "    return pZ\n",
    "\n",
    "def computeP(p,rho,g):\n",
    "    num = norm.ppf(p)-np.multiply(np.sqrt(rho),g)\n",
    "    pG = norm.cdf(np.divide(num,np.sqrt(1-rho)))\n",
    "    return pG\n",
    "\n",
    "def jointDefaultProbabilityT(p,q,myRho,nu):\n",
    "    lowerBound = np.maximum(nu-40,2)\n",
    "    support = [[-10,10],[lowerBound,nu+40]]\n",
    "    pr,err=nInt.nquad(jointIntegrandT,support,args=(p,q,myRho,nu))\n",
    "    return pr\n",
    "\n",
    "def jointDefaultProbabilityG(p,q,myRho):\n",
    "    pr,err=nInt.quad(jointIntegrandG,-10,10,args=(p,q,myRho))\n",
    "    return pr\n",
    "\n",
    "def jointIntegrandT(g,w,p,q,myRho,nu):\n",
    "    p1 = computeP_t(p,myRho,g,w,nu)\n",
    "    p2 = computeP_t(q,myRho,g,w,nu)\n",
    "    density1 = util.gaussianDensity(g,0,1)\n",
    "    density2 = util.chi2Density(w,nu)    \n",
    "    f = p1*p2*density1*density2\n",
    "    return f\n",
    "\n",
    "def jointIntegrandG(g,p,q,myRho):\n",
    "    p1 = computeP(p,myRho,g)\n",
    "    p2 = computeP(q,myRho,g)\n",
    "    density = util.gaussianDensity(g,0,1)\n",
    "    f = p1*p2*density\n",
    "    return f\n",
    "\n",
    "def bivariateTDensity(x1,x2,rho,nu,d=2):\n",
    "    Sigma = np.array([[1,rho],[rho,1]])\n",
    "    myX = np.array([x1,x2])\n",
    "    t1 = math.gamma((nu+d)/2)\n",
    "    t2 = math.gamma(nu/2) \n",
    "    t3 = np.power(nu*math.pi,d/2)\n",
    "    t4 = np.sqrt(anp.det(Sigma))\n",
    "    constant = np.divide(t1,t2*t3*t4)\n",
    "    t5 = np.dot(np.dot(myX,anp.inv(Sigma)),myX)\n",
    "    integrand = constant*np.power(1+t5/nu,-(nu+d)/2)\n",
    "    return integrand\n",
    "\n",
    "def bivariateTCdf(yy,xx,rho,nu):    \n",
    "    t_ans, err = nInt.dblquad(bivariateTDensity, -10, xx,\n",
    "                   lambda x: -10,\n",
    "                   lambda x: yy,args=(rho,nu))\n",
    "    return t_ans\n",
    "\n",
    "def bivariateGDensity(x1,x2,rho):\n",
    "    S = np.array([[1,rho],[rho,1]])\n",
    "    t1 = 2*math.pi*np.sqrt(anp.det(S))\n",
    "    t2 = np.dot(np.dot(np.array([x1,x2]),anp.inv(S)),np.array([x1,x2]))\n",
    "    return np.divide(1,t1)*np.exp(-t2/2)\n",
    "\n",
    "\n",
    "def buildAssetCorrelationMatrix(a,b,regionId):\n",
    "    J = len(b)\n",
    "    R = np.zeros([J,J])\n",
    "    for n in range(0,J):\n",
    "        for m in range(0,J):\n",
    "            if regionId[n]==regionId[m]:\n",
    "                R[n,m] = a + (1-a)*np.sqrt(b[n]*b[m])\n",
    "            else:\n",
    "                R[n,m] = a\n",
    "    return R\n",
    "\n",
    "def buildDefaultCorrelationMatrix(a,b,pMean,regionId,nu):\n",
    "    J = len(regionId)\n",
    "    R = buildAssetCorrelationMatrix(a,b,regionId)    \n",
    "    D = np.zeros([J,J])\n",
    "    for n in range(0,J):\n",
    "        p_n = pMean[n]\n",
    "        for m in range(0,J):\n",
    "            p_m = pMean[m]\n",
    "            p_nm = bivariateTCdf(norm.ppf(p_n),norm.ppf(p_m),R[n,m],nu)\n",
    "            D[n,m] = (p_nm - p_n*p_m)/math.sqrt(p_n*(1-p_n)*p_m*(1-p_m))\n",
    "    return D\n",
    "\n",
    "def calibrateOF(x,B,pMean,regionId,nu):\n",
    "    a = x[0]\n",
    "    b = np.array([x[1],x[2],x[3]])\n",
    "    D = buildDefaultCorrelationMatrix(a,b,pMean,regionId,nu)     \n",
    "    f = anp.norm(D-B,ord='fro')\n",
    "    return f\n",
    "\n",
    "def calibrateMFT(B,pMean,regionId,nu):\n",
    "    myBounds = ((0.001,0.30),(0.001,0.30),(0.001,0.30),\n",
    "                (0.001,0.30))                            \n",
    "    M = 100\n",
    "    xRandom = np.random.uniform(0,0.30,[M,4])\n",
    "    functionValues = np.zeros(M)\n",
    "    for m in range(0,M):\n",
    "        functionValues[m] = calibrateOF(xRandom[m,:],B,pMean,regionId,nu)\n",
    "    newOF=np.min(functionValues)\n",
    "    xStart = xRandom[functionValues==newOF]\n",
    "    xhat = scipy.optimize.minimize(calibrateOF, \n",
    "                    xStart, args=(B,pMean,regionId,nu), \n",
    "                    method='SLSQP', jac=None, bounds=myBounds)   \n",
    "    return xhat    \n",
    "\n",
    "def calibrateT(x,myP,targetRho,nu):\n",
    "    jointDefaultProb = jointDefaultProbabilityT(myP,myP,x,nu)\n",
    "    defaultCorrelation = np.divide(jointDefaultProb-myP**2,myP*(1-myP))\n",
    "    return np.abs(defaultCorrelation-targetRho)\n",
    "\n",
    "def thresholdCalibrationGridSearch(dGrid,myP,rhoTarget,whichModel,nu=30):\n",
    "    jointDefaultProb = np.zeros([2,25])\n",
    "    dEstimate = np.zeros([2,25])\n",
    "    for n in range(0,len(dGrid)):\n",
    "        print(\"Iteration %d\" % (n+1))\n",
    "        if whichModel==1:\n",
    "            support = [[-8,norm.ppf(myP)],[-8,norm.ppf(myP)]]\n",
    "            jointDefaultProb[0,n] = jointDefaultProbabilityG(myP,myP,dGrid[n])\n",
    "            jointDefaultProb[1,n],err = nInt.nquad(bivariateGDensity,support,args=(dGrid[n],2))\n",
    "        elif whichModel==2:   \n",
    "            support = [[-8,myT.ppf(myP,nu)],[-8,myT.ppf(myP,nu)]]\n",
    "            jointDefaultProb[0,n] = jointDefaultProbabilityT(myP,myP,dGrid[n],nu)\n",
    "            jointDefaultProb[1,n],err = nInt.nquad(bivariateTDensity,support,args=(dGrid[n],nu))\n",
    "        dEstimate[0,n] = np.divide(jointDefaultProb[0,n]-myP**2,myP*(1-myP))\n",
    "        dEstimate[1,n] = np.divide(jointDefaultProb[1,n]-myP**2,myP*(1-myP))\n",
    "    print(\"The conditonal approach gives %0.2f\" % (np.interp(rhoTarget,dEstimate[0,:],dGrid)))\n",
    "    print(\"The classic approach gives %0.2f\" % (np.interp(rhoTarget,dEstimate[1,:],dGrid)))\n",
    "    return dEstimate\n",
    "\n",
    "def tTailDependenceCoefficient(rho,nu):\n",
    "    a = -np.sqrt(np.divide((nu+1)*(1-rho),1+rho)) \n",
    "    tCoefficient = 2*myT.cdf(a,nu+1)    \n",
    "    return tCoefficient\n",
    "\n",
    "def tCalibrate(x,myP,rhoTarget,tdTarget):\n",
    "    if (x[0]<=0) | (x[1]<=0):\n",
    "        return [100, 100]\n",
    "    jointDefaultProb = jointDefaultProbabilityT(myP,myP,x[0],x[1])\n",
    "    rhoValue = np.divide(jointDefaultProb-myP**2,myP*(1-myP))\n",
    "    tdValue = tTailDependenceCoefficient(x[0],x[1])\n",
    "    f1 = rhoValue - rhoTarget    \n",
    "    f2 = tdValue - tdTarget    \n",
    "    return [f1, f2]\n",
    "\n",
    "def getMultiFactorY(N,M,p,a,b,rId,nu,isT):\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    regions = np.random.normal(0,1,[M,len(np.unique(rId))]) \n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    R = regions[:,rId]\n",
    "    A = np.tile(a*np.ones(N),(M,1))\n",
    "    B = np.tile(b[rId],(M,1))\n",
    "    T0 = np.multiply(np.sqrt(A),G)\n",
    "    T1 = np.sqrt(1-A)\n",
    "    T2 = np.multiply(np.sqrt(B),R) + np.multiply(np.sqrt(1-B),e)\n",
    "    if isT==1: \n",
    "        W = np.transpose(np.sqrt(nu/np.tile(np.random.chisquare(nu,M),(N,1))))\n",
    "        return np.multiply(W,T0+np.multiply(T1,T2))\n",
    "    else: \n",
    "        return T0+np.multiply(T1,T2)\n",
    "\n",
    "def multiFactorThresholdModel(N,M,a,b,rId,p,c,nu,alpha,isT):\n",
    "    Y = getMultiFactorY(N,M,p,a,b,rId,nu,isT)\n",
    "    if isT==1:\n",
    "        K = myT.ppf(p,nu)*np.ones((M,1)) \n",
    "    else:\n",
    "        K = norm.ppf(p)*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es      \n",
    "\n",
    "def nvmDensity(v,x,myA,whichModel):\n",
    "    t1 = np.divide(1,np.sqrt(2*math.pi*v))\n",
    "    t2 = np.exp(-np.divide(x**2,2*v))\n",
    "    if whichModel==0:\n",
    "        return t1*t2*util.gammaDensity(v,myA,myA)\n",
    "    elif whichModel==1:\n",
    "        return t1*t2*util.gigDensity(v,myA)\n",
    "    \n",
    "def nvmPdf(x,myA,whichModel):\n",
    "    f,err = nInt.quad(nvmDensity,0,50,args=(x,myA,whichModel)) \n",
    "    return f\n",
    "\n",
    "def nvmCdf(x,myA,whichModel):\n",
    "    F,err = nInt.quad(nvmPdf,-8,x,args=(myA,whichModel)) \n",
    "    return F    \n",
    "\n",
    "def nvmTarget(x,myVal,myA,whichModel):\n",
    "    F,err = nInt.quad(nvmPdf,-8,x,args=(myA,whichModel)) \n",
    "    return F-myVal\n",
    "\n",
    "def nvmPpf(myVal,myA,whichModel):\n",
    "    r = scipy.optimize.fsolve(nvmTarget,0,args=(myVal,myA,whichModel))\n",
    "    return r[0]        \n",
    "\n",
    "def getNVMY(N,M,rho,myA,whichModel):\n",
    "    G = np.transpose(np.tile(np.random.normal(0,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    if whichModel==0:\n",
    "        V = np.transpose(np.sqrt(np.tile(np.random.gamma(myA,1/myA,M),(N,1))))\n",
    "    elif whichModel==1:\n",
    "        V = np.transpose(np.sqrt(np.tile(gig.rgig(M,1,myA,myA),(N,1))))\n",
    "    Y = np.multiply(V,math.sqrt(rho)*G + math.sqrt(1-rho)*e)\n",
    "    return Y   \n",
    "\n",
    "def computeP_NVM(p,rho,y,v,myA,invCdf):\n",
    "    num = np.sqrt(1/v)*invCdf-np.multiply(np.sqrt(rho),y)\n",
    "    pZ = norm.cdf(np.divide(num,np.sqrt(1-rho)))\n",
    "    return pZ\n",
    "  \n",
    "def nvmKurtosis(rho,myA,whichModel):\n",
    "    if whichModel==0:\n",
    "        return 3*(1+myA)/myA\n",
    "    elif whichModel==1:\n",
    "        num = scipy.special.kn(3, myA)*scipy.special.kn(1, myA)\n",
    "        den = scipy.special.kn(2, myA)**2\n",
    "        return 3*np.divide(num,den)  \n",
    "       \n",
    "def ghVariance(myA):\n",
    "    return  scipy.special.kn(2, myA)/scipy.special.kn(1, myA)   \n",
    "        \n",
    "def jointDefaultProbabilityNVM(p,q,myRho,myA,whichModel):    \n",
    "    invCdf = nvmPpf(p,myA,whichModel)\n",
    "    support = [[-8,8],[0,100]]\n",
    "    pr,err=nInt.nquad(jointIntegrandNVM,support,args=(p,q,myRho,myA,invCdf,whichModel))\n",
    "    return pr\n",
    "\n",
    "def jointIntegrandNVM(g,v,p,q,myRho,myA,invCdf,whichModel):\n",
    "    p1 = computeP_NVM(p,myRho,g,v,myA,invCdf)\n",
    "    p2 = computeP_NVM(q,myRho,g,v,myA,invCdf)\n",
    "    density1 = util.gaussianDensity(g,0,1)\n",
    "    if whichModel==0:\n",
    "        density2 = util.gammaDensity(v,myA,myA)            \n",
    "    elif whichModel==1:\n",
    "        density2 = util.gigDensity(v,myA)    \n",
    "    return p1*p2*density1*density2         \n",
    "        \n",
    "def nvmCalibrate(x,myP,rhoTarget,kTarget,whichModel):\n",
    "    if (x[0]<=0) | (x[1]<=0):\n",
    "        return [100, 100]\n",
    "    jointDefaultProb = jointDefaultProbabilityNVM(myP,myP,x[0],x[1],whichModel)\n",
    "    rhoValue = np.divide(jointDefaultProb-myP**2,myP*(1-myP))\n",
    "    kValue = nvmKurtosis(x[0],x[1],whichModel)\n",
    "    f1 = rhoValue - rhoTarget    \n",
    "    f2 = kValue - kTarget    \n",
    "    return [f1, f2]\n",
    "\n",
    "def oneFactorNVMModel(N,M,p,c,rho,myA,alpha,whichModel):\n",
    "    Y = getNVMY(N,M,rho,myA,whichModel)\n",
    "    invVector = np.zeros(N)\n",
    "    for n in range(0,N): \n",
    "        invVector[n] = nvmPpf(p[n],myA,whichModel)    \n",
    "    K = invVector*np.ones((M,1))        \n",
    "    lossIndicator = 1*np.less(Y,K)     \n",
    "    lossDistribution = np.sort(np.dot(lossIndicator,c),axis=None)\n",
    "    el,ul,var,es=util.computeRiskMeasures(M,lossDistribution,alpha)\n",
    "    return el,ul,var,es      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b7782-c635-42fc-bb00-cc2c8eb97d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "import cmUtilities as util\n",
    "import binomialPoissonModels as bp\n",
    "import importlib \n",
    "from scipy.stats import norm\n",
    "import scipy.integrate as nInt\n",
    "import thresholdModels as th\n",
    "from scipy.stats import t as myT\n",
    "\n",
    "importlib.reload(util)\n",
    "importlib.reload(th)\n",
    "importlib.reload(bp)\n",
    "\n",
    "def mcThresholdTDecomposition(N,M,S,p,c,rho,nu,isT,myAlpha):\n",
    "    contributions = np.zeros([N,S,2])\n",
    "    var = np.zeros(S)\n",
    "    es = np.zeros(S)\n",
    "    K = myT.ppf(p,nu)*np.ones((M,1))        \n",
    "    for s in range(0,S):\n",
    "        print(\"Iteration: %d\" % (s+1))\n",
    "        Y = th.getY(N,M,p,rho,nu,isT)\n",
    "        myD = 1*np.less(Y,K)     \n",
    "        myLoss = np.sort(np.dot(myD,c),axis=None)\n",
    "        el,ul,var[s],es[s]=util.computeRiskMeasures(M,myLoss,np.array([myAlpha]))\n",
    "        varVector = c*myD[np.dot(myD,c)==var[s],:]\n",
    "        esVector = c*myD[np.dot(myD,c)>=var[s],:]\n",
    "        contributions[:,s,0] = np.sum(varVector,0)/varVector.shape[0]\n",
    "        contributions[:,s,1] = np.sum(esVector,0)/esVector.shape[0]\n",
    "    return contributions,var,es\n",
    "\n",
    "def mcThresholdGDecomposition(N,M,S,p,c,rho,nu,isT,myAlpha):\n",
    "    contributions = np.zeros([N,S,2])\n",
    "    var = np.zeros(S)\n",
    "    es = np.zeros(S)\n",
    "    K = norm.ppf(p)*np.ones((M,1))        \n",
    "    for s in range(0,S):\n",
    "        print(\"Iteration: %d\" % (s+1))\n",
    "        Y = th.getY(N,M,p,rho,nu,isT)\n",
    "        myD = 1*np.less(Y,K)     \n",
    "        myLoss = np.sort(np.dot(myD,c),axis=None)\n",
    "        el,ul,var[s],es[s]=util.computeRiskMeasures(M,myLoss,np.array([myAlpha]))\n",
    "        varVector = c*myD[np.dot(myD,c)==var[s],:]\n",
    "        esVector = c*myD[np.dot(myD,c)>=var[s],:]\n",
    "        contributions[:,s,0] = np.sum(varVector,0)/varVector.shape[0]\n",
    "        contributions[:,s,1] = np.sum(esVector,0)/esVector.shape[0]\n",
    "    return contributions,var,es\n",
    "\n",
    "\n",
    "def mcThresholdIndDecomposition(N,M,S,p,c,myAlpha):\n",
    "    contributions = np.zeros([N,S,2])\n",
    "    var = np.zeros(S)\n",
    "    es = np.zeros(S)\n",
    "    for s in range(0,S):\n",
    "        print(\"Iteration: %d\" % (s+1))\n",
    "        myLoss,myD = bp.independentBinomialLossDistribution(N,M,p,c,myAlpha,1)\n",
    "        el,ul,var[s],es[s]=util.computeRiskMeasures(M,myLoss,np.array([myAlpha]))\n",
    "        varVector = c*myD[np.dot(myD,c)==var[s],:]\n",
    "        esVector = c*myD[np.dot(myD,c)>=var[s],:]\n",
    "        contributions[:,s,0] = np.sum(varVector,0)/varVector.shape[0]\n",
    "        contributions[:,s,1] = np.sum(esVector,0)/esVector.shape[0]\n",
    "    return contributions,var,es\n",
    "\n",
    "def computeMGF(t,p,c): \n",
    "    return 1-p+p*np.exp(c*t)\n",
    "    \n",
    "def computeCGF(t,p,c):\n",
    "    return np.sum(np.log(computeMGF(t,p,c)))\n",
    " \n",
    "def computeCGF_1(t,p,c):\n",
    "    num = c*p*np.exp(c*t)\n",
    "    den = computeMGF(t,p,c)\n",
    "    return np.sum(np.divide(num,den))\n",
    "\n",
    "def computeCGF_2(t,p,c,asVector=0):\n",
    "    num = (1-p)*(c**2)*p*np.exp(c*t)\n",
    "    den = np.power(computeMGF(t,p,c),2)\n",
    "    if asVector==1:\n",
    "        return np.divide(num,den)\n",
    "    else:\n",
    "        return np.sum(np.divide(num,den))\n",
    "\n",
    "def computeCGF_3(t,p,c):\n",
    "    num1 = (1-p)*(c**3)*p*np.exp(c*t)\n",
    "    num2 = 2*(1-p)*(c**3)*(p**2)*np.exp(2*c*t)\n",
    "    den1 = np.power(computeMGF(t,p,c),2)\n",
    "    den2 = np.power(computeMGF(t,p,c),3)\n",
    "    return np.sum(np.divide(num1,den1)-np.divide(num2,den2))\n",
    "\n",
    "def getSaddlePoint(p,c,l,startPoint=0.00025):\n",
    "    r = scipy.optimize.root(computeCGFRoot,startPoint,args=(p,c,l),method='hybr')\n",
    "    return r.x    \n",
    "\n",
    "def step(x):\n",
    "    return 1 * (x > 0)\n",
    "\n",
    "def computeCGFRoot(t,p,c,l):\n",
    "    return computeCGF_1(t,p,c)-l\n",
    "\n",
    "def getJ(l,p,c,t_l,myOrder):\n",
    "    K2 = computeCGF_2(t_l,p,c)\n",
    "    if myOrder==0:\n",
    "        return np.sqrt(np.divide(1,2*math.pi*K2))\n",
    "    if myOrder==1:\n",
    "        t0 = K2*(t_l**2)\n",
    "        return np.sign(t_l)*np.exp(0.5*t0)*norm.cdf(-np.sqrt(t0))\n",
    "    if myOrder==2:\n",
    "        return K2*(getJ(l,p,c,t_l,0)-t_l*getJ(l,p,c,t_l,1))\n",
    "\n",
    "def saddlePointDensity(l,p,c):\n",
    "    t_l = getSaddlePoint(p,c,l)\n",
    "    return np.exp(computeCGF(t_l,p,c)-t_l*l)*getJ(l,p,c,t_l,0)\n",
    "\n",
    "def saddlePointTailProbability(l,p,c):\n",
    "    t_l = getSaddlePoint(p,c,l)\n",
    "    return np.exp(computeCGF(t_l,p,c)-t_l*l)*getJ(l,p,c,t_l,1)\n",
    "    \n",
    "def saddlePointShortfallIntegral(l,p,c):\n",
    "    den = saddlePointTailProbability(l,p,c)\n",
    "    t_l = getSaddlePoint(p,c,l)\n",
    "    return l + np.exp(computeCGF(t_l,p,c)-t_l*l)*getJ(l,p,c,t_l,2)/den\n",
    "    \n",
    "def identifyVaRInd(x,p,c,myAlpha):\n",
    "    tpY = saddlePointTailProbability(x,p,c)\n",
    "    return 1e4*np.power((1-tpY)-myAlpha,2)\n",
    "\n",
    "def getVaRC(l,p,c):\n",
    "    t_l = getSaddlePoint(p,c,l)\n",
    "    return np.divide(c*p*np.exp(c*t_l),computeMGF(t_l,p,c))\n",
    "    \n",
    "def getESC(l,p,c):\n",
    "    varPart = getVaRC(l,p,c)\n",
    "    myAlpha = saddlePointTailProbability(l,p,c)\n",
    "    t_l = getSaddlePoint(p,c,l)\n",
    "    K2 = computeCGF_2(t_l,p,c)\n",
    "    myW = computeCGF_2(t_l,p,c,1)\n",
    "    t0 = np.exp(computeCGF(t_l,p,c)-t_l*l)*getJ(l,p,c,t_l,2)\n",
    "    return varPart + np.divide(t0*np.divide(myW,K2),myAlpha)    \n",
    "\n",
    "def saddlePointApprox(l,p,c,t_l,myDegree,constant=0):\n",
    "    if myDegree==1:\n",
    "        constant = step(-t_l)\n",
    "    elif myDegree==2:\n",
    "        constant = step(-t_l)*(np.dot(p,c)-l)\n",
    "    coefficient = np.exp(computeCGF(t_l,p,c)-t_l*l)\n",
    "    return  constant + coefficient*getJ(l,p,c,t_l,myDegree)\n",
    "    \n",
    "def getPy(p,y,p1,p2,whichModel,v=0):\n",
    "    if whichModel==0: # Gaussian threshold\n",
    "        return th.computeP(p,p1,y)\n",
    "    elif whichModel==1: # beta\n",
    "        return y*np.ones(len(p))\n",
    "    elif whichModel==2: # CreditRisk+\n",
    "        v = p*(1-p1+p1*y)\n",
    "        return np.maximum(np.minimum(1-np.exp(-v),0.999),0.0001)\n",
    "    elif whichModel==3: # logit\n",
    "        return np.reciprocal(1+np.exp(-(p1+p2*y)))\n",
    "    elif whichModel==4: # probit\n",
    "        return norm.ppf(p1+p2*y)    \n",
    "    elif whichModel==5: # Weibull\n",
    "        return np.maximum(np.minimum(1-np.exp(-y),0.999),0.0001)*np.ones(len(p))\n",
    "    if whichModel==6: # t threshold\n",
    "        return th.computeP_t(p,p1,y,v,p2)\n",
    "    \n",
    "def getYDensity(y,p1,p2,whichModel,v=0):\n",
    "    if whichModel==0: \n",
    "        return util.gaussianDensity(y,0,1)\n",
    "    elif whichModel==1:\n",
    "        return util.betaDensity(y,p1,p2)\n",
    "    elif whichModel==2:\n",
    "        return util.gammaDensity(y,p2,p2)    \n",
    "    elif whichModel==3:\n",
    "        return util.gaussianDensity(y,0,1)\n",
    "    elif whichModel==4:\n",
    "        return util.gaussianDensity(y,0,1)\n",
    "    elif whichModel==5:\n",
    "        return util.weibullDensity(y,p1,p2)   \n",
    "    elif whichModel==6:\n",
    "        return util.gaussianDensity(y,0,1)*util.chi2Density(v,p2)   \n",
    "\n",
    "def computeYIntegral(y,l,p,c,p1,p2,whichModel,myDegree):\n",
    "    pY = getPy(p,y,p1,p2,whichModel)\n",
    "    d = getYDensity(y,p1,p2,whichModel)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    return saddlePointApprox(l,pY,c,t_l,myDegree)*d  \n",
    "\n",
    "def identifyVaR(x,p,c,p1,p2,whichModel,myAlpha):\n",
    "    tpY = myApprox(x,p,c,p1,p2,whichModel,1)\n",
    "    return 1e6*np.power((1-tpY)-myAlpha,2)\n",
    "\n",
    "def getIntegrationBounds(whichModel):\n",
    "    if whichModel==0:\n",
    "        lB,uB = -8,8\n",
    "    elif whichModel==1:\n",
    "        lB,uB = 0.0001,0.9999\n",
    "    elif whichModel==2:\n",
    "        lB,uB = 0.0001,100\n",
    "    if whichModel==3:\n",
    "        lB,uB = -8,8\n",
    "    if whichModel==4:\n",
    "        lB,uB = -8,8\n",
    "    elif whichModel==5:\n",
    "        lB,uB = 0.0001,35\n",
    "    return lB,uB\n",
    "\n",
    "def myApprox(l,p,c,p1,p2,whichModel,myDegree,constant=0,den=1):\n",
    "    lB,uB = getIntegrationBounds(whichModel)\n",
    "    if myDegree==2:\n",
    "        constant = l\n",
    "        den,err = nInt.quad(computeYIntegral,lB,uB,args=(l,p,c,p1,p2,whichModel,1))        \n",
    "    num,err = nInt.quad(computeYIntegral,lB,uB,args=(l,p,c,p1,p2,whichModel,myDegree))\n",
    "    return constant + np.divide(num,den) \n",
    "\n",
    "def varCNumerator(y,l,myN,p,c,p1,p2,whichModel,v=0):\n",
    "    pY = getPy(p,y,p1,p2,whichModel,v)\n",
    "    d = getYDensity(y,p1,p2,whichModel,v)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    num = pY[myN]*np.exp(c[myN]*t_l)\n",
    "    den = computeMGF(t_l,pY[myN],c[myN])\n",
    "    return np.divide(num,den)*saddlePointApprox(l,pY,c,t_l,0)*d\n",
    "\n",
    "def myVaRCY(l,p,c,p1,p2,whichModel):\n",
    "    lB,uB = getIntegrationBounds(whichModel)\n",
    "    den = myApprox(l,p,c,p1,p2,whichModel,0)\n",
    "    num = np.zeros(len(p))\n",
    "    for n in range(0,len(p)):\n",
    "        num[n],err = nInt.quad(varCNumerator,lB,uB,args=(l,n,p,c,p1,p2,whichModel))\n",
    "    return c*np.divide(num,den)\n",
    "\n",
    "def esCVaR(y,l,myN,p,c,p1,p2,whichModel,myAlpha,extraTerm=0):\n",
    "    pY = getPy(p,y,p1,p2,whichModel)\n",
    "    d = getYDensity(y,p1,p2,whichModel)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    baseTerm = np.divide(pY[myN]*np.exp(c[myN]*t_l),computeMGF(t_l,pY[myN],c[myN]))\n",
    "    if t_l<0:\n",
    "        extraTerm = (pY[myN]-baseTerm)/myAlpha\n",
    "    return (baseTerm+extraTerm)*d \n",
    "\n",
    "def myESCY(l,p,c,p1,p2,whichModel):\n",
    "    lB,uB = getIntegrationBounds(whichModel)\n",
    "    myAlpha = myApprox(l,p,c,p1,p2,whichModel,1)\n",
    "    esC = np.zeros(len(p))\n",
    "    for n in range(0,len(p)):\n",
    "        esC[n],err = nInt.quad(integrateAll,lB,uB,args=(l,n,p,c,p1,p2,whichModel))\n",
    "    return (1/myAlpha)*esC\n",
    "\n",
    "def integrateAll(y,l,n,p,c,p1,p2,whichModel):\n",
    "    pY = getPy(p,y,p1,p2,whichModel)\n",
    "    d = getYDensity(y,p1,p2,whichModel)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    varPart = getVaRPart(l,n,pY,c,t_l)\n",
    "    esPart = getESPart(l,n,pY,c,t_l)\n",
    "    correctPart = getCorrectionPart(l,n,pY,c,t_l)\n",
    "    return (varPart + esPart + correctPart)*d\n",
    "\n",
    "def getVaRPart(l,myN,pY,c,t_l):\n",
    "    baseTerm = np.divide(pY[myN]*np.exp(c[myN]*t_l),computeMGF(t_l,pY[myN],c[myN]))\n",
    "    return c[myN]*baseTerm*saddlePointApprox(l,pY,c,t_l,1)\n",
    "\n",
    "def getESPart(l,myN,pY,c,t_l):\n",
    "    K2 = computeCGF_2(t_l,pY,c)\n",
    "    myW = computeCGF_2(t_l,pY,c,1)\n",
    "    t0 = np.exp(computeCGF(t_l,pY,c)-t_l*l)*getJ(l,pY,c,t_l,2)\n",
    "    return np.divide(t0*np.divide(myW[myN],K2),1)    \n",
    "\n",
    "def getCorrectionPart(l,myN,pY,c,t_l):\n",
    "    t0 = c[myN]*pY[myN]\n",
    "    t1 = computeCGF_1(t_l,pY[myN],c[myN])\n",
    "    return step(-t_l)*(t0-t1)\n",
    "\n",
    "def computeYIntegralT(y,v,l,p,c,p1,p2,whichModel,myDegree):\n",
    "    pY = getPy(p,y,p1,p2,whichModel,v)\n",
    "    d = getYDensity(y,p1,p2,whichModel,v)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    return saddlePointApprox(l,pY,c,t_l,myDegree)*d  \n",
    "\n",
    "def myApproxT(l,p,c,p1,p2,whichModel,myDegree,constant=0,den=1):\n",
    "    lowerBound = np.maximum(p2-20,0.0001)\n",
    "    support = [[-8,8],[lowerBound,p2+8]]\n",
    "    if myDegree==2:\n",
    "        constant = l\n",
    "        den,err = nInt.nquad(computeYIntegralT,support,args=(l,p,c,p1,p2,whichModel,1))        \n",
    "    num,err = nInt.nquad(computeYIntegralT,support,args=(l,p,c,p1,p2,whichModel,myDegree))\n",
    "    return constant + np.divide(num,den) \n",
    "\n",
    "def identifyVaRT(x,p,c,p1,p2,whichModel,myAlpha):\n",
    "    tpY = myApproxT(x,p,c,p1,p2,whichModel,1)\n",
    "    return 1e6*np.power((1-tpY)-myAlpha,2)\n",
    "\n",
    "def varCNumeratorT(y,v,l,myN,p,c,p1,p2,whichModel):\n",
    "    pY = getPy(p,y,p1,p2,whichModel,v)\n",
    "    d = getYDensity(y,p1,p2,whichModel,v)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    num = pY[myN]*np.exp(c[myN]*t_l)\n",
    "    den = computeMGF(t_l,pY[myN],c[myN])\n",
    "    return np.divide(num,den)*saddlePointApprox(l,pY,c,t_l,0)*d\n",
    "\n",
    "def myVaRCYT(l,p,c,p1,p2,whichModel):\n",
    "    lowerBound = np.maximum(p2-20,0.0001)\n",
    "    support = [[-8,8],[lowerBound,p2+8]]\n",
    "    den = myApproxT(l,p,c,p1,p2,whichModel,0)\n",
    "    num = np.zeros(len(p))\n",
    "    for n in range(0,len(p)):\n",
    "        num[n],err = nInt.nquad(varCNumeratorT,support,args=(l,n,p,c,p1,p2,whichModel))\n",
    "    return c*np.divide(num,den)\n",
    "\n",
    "def myESCYT(l,p,c,p1,p2,whichModel):\n",
    "    lowerBound = np.maximum(p2-20,0.0001)\n",
    "    support = [[-8,8],[lowerBound,p2+8]]\n",
    "    myAlpha = myApproxT(l,p,c,p1,p2,whichModel,1)\n",
    "    esC = np.zeros(len(p))\n",
    "    for n in range(0,len(p)):\n",
    "        esC[n],err = nInt.nquad(integrateAllT,support,args=(l,n,p,c,p1,p2,whichModel))\n",
    "    return (1/myAlpha)*esC\n",
    "\n",
    "def integrateAllT(y,v,l,n,p,c,p1,p2,whichModel):\n",
    "    pY = getPy(p,y,p1,p2,whichModel,v)\n",
    "    d = getYDensity(y,p1,p2,whichModel,v)\n",
    "    t_l = getSaddlePoint(pY,c,l)\n",
    "    varPart = getVaRPart(l,n,pY,c,t_l)\n",
    "    esPart = getESPart(l,n,pY,c,t_l)\n",
    "    correctPart = getCorrectionPart(l,n,pY,c,t_l)\n",
    "    return (varPart + esPart + correctPart)*d\n",
    "\n",
    "def findAlphaGaussian(a,N,M,p,c,l,myRho):\n",
    "    elTemp,ulTemp,varTemp,esTemp = th.oneFactorThresholdModel(N,M,p,c,\n",
    "                         myRho,0,np.array([a]),0)\n",
    "    return 1e4*(l-esTemp[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d49a9e-21aa-493f-936b-7294d889f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cmUtilities as util\n",
    "import importlib \n",
    "from scipy.stats import norm\n",
    "from scipy.stats import t as myT\n",
    "import scipy\n",
    "\n",
    "import thresholdModels as th\n",
    "import varContributions as vc\n",
    "\n",
    "importlib.reload(th)\n",
    "importlib.reload(vc)\n",
    "importlib.reload(util)\n",
    "\n",
    "def myFunction(x):\n",
    "    return 7*np.sin(5*x) + 5*np.power(x,3) + 3*np.exp(x/2)\n",
    "\n",
    "def mcIntegration(M):\n",
    "    U = np.random.uniform(0,1,M)\n",
    "    return np.mean(myFunction(U))\n",
    "\n",
    "def naiveNumericalIntegration(K,a,b):\n",
    "    myGrid = np.linspace(a,b,K)\n",
    "    myIntegral = 0\n",
    "    for n in range(0,K-1):\n",
    "        dx = myGrid[n+1]-myGrid[n]\n",
    "        myIntegral += myFunction(myGrid[n+1])*dx\n",
    "    return myIntegral    \n",
    "\n",
    "def getQ(theta,c,p):\n",
    "    return np.divide(p*np.exp(c*theta),vc.computeMGF(theta,p,c))\n",
    "    \n",
    "def meanShiftOF(mu,c,p,l,myRho):\n",
    "    pZ = th.computeP(p,myRho,mu)\n",
    "    theta = vc.getSaddlePoint(pZ,c,l,0.0)\n",
    "    f_l = -theta*l + vc.computeCGF(theta,pZ,c)\n",
    "    return -(f_l - 0.5*np.dot(mu,mu))    \n",
    "\n",
    "def getOptimalMeanShift(c,p,l,myRho):\n",
    "    r = scipy.optimize.minimize(meanShiftOF,-1.0,args=(c,p,l,myRho), \n",
    "                            method='SLSQP',jac=None,bounds=[(-4.0,4.0)]) \n",
    "    return r.x\n",
    "\n",
    "def isIndDefault(N,M,p,c,l):\n",
    "    U = np.random.uniform(0,1,[M,N])\n",
    "    theta = vc.getSaddlePoint(p,c,l,0.0)\n",
    "    qZ = getQ(theta,c,p)\n",
    "    cgf = vc.computeCGF(theta,p,c)\n",
    "    I = np.transpose(1*np.less(U,qZ))\n",
    "    L = np.dot(c,I)\n",
    "    rn = computeRND(theta,L,cgf)        \n",
    "    tailProb = np.mean(np.multiply(L>l,rn)) \n",
    "    eShortfall =  np.mean(np.multiply(L*(L>l),rn))/tailProb        \n",
    "    return tailProb,eShortfall    \n",
    "\n",
    "def isThreshold(N,M,p,c,l,myRho,nu,shiftMean,isT,invVector=0):\n",
    "    mu = 0.0\n",
    "    gamma = 0.0\n",
    "    if shiftMean==1:\n",
    "        mu = getOptimalMeanShift(c,p,l,myRho)\n",
    "    theta = np.zeros(M)\n",
    "    cgf = np.zeros(M)\n",
    "    qZ = np.zeros([M,N])\n",
    "    G = np.transpose(np.tile(np.random.normal(mu,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    if isT==1:\n",
    "        gamma = -2\n",
    "        W = np.random.chisquare(nu,M)\n",
    "        myV = W/(1-2*gamma)\n",
    "        V = np.transpose(np.sqrt(np.tile(myV,(N,1))/nu))\n",
    "        num = (1/V)*myT.ppf(p,nu)*np.ones((M,1))-np.multiply(np.sqrt(myRho),G)\n",
    "        pZ = norm.cdf(np.divide(num,np.sqrt(1-myRho)))\n",
    "    elif isT==2:\n",
    "        V = np.transpose(np.sqrt(np.tile(np.random.gamma(nu,1/nu,M),(N,1))))\n",
    "        num = (1/V)*invVector*np.ones((M,1))-np.multiply(np.sqrt(myRho),G)\n",
    "        pZ = norm.cdf(np.divide(num,np.sqrt(1-myRho)))\n",
    "    else:\n",
    "        pZ = th.computeP(p,myRho,G)\n",
    "    for n in range(0,M):\n",
    "        theta[n] = vc.getSaddlePoint(pZ[n,:],c,l,0.0)\n",
    "        qZ[n,:] = getQ(theta[n],c,pZ[n,:])\n",
    "        cgf[n] = vc.computeCGF(theta[n],pZ[n,:],c)\n",
    "    I = np.transpose(1*np.less(e,norm.ppf(qZ)))\n",
    "    L = np.dot(c,I)\n",
    "    if isT==1:\n",
    "        rnChi = np.exp(-gamma*myV-(nu/2)*np.log(1-2*gamma))\n",
    "    else:\n",
    "        rnChi = np.ones(M)\n",
    "    if shiftMean==1:\n",
    "        rn = computeRND(theta,L,cgf)*np.exp(-mu*G[:,0]+0.5*(mu**2))*rnChi\n",
    "    else:\n",
    "        rn = computeRND(theta,L,cgf)*rnChi\n",
    "    tailProb = np.mean(np.multiply(L>l,rn)) \n",
    "    eShortfall =  np.mean(np.multiply(L*(L>l),rn))/tailProb        \n",
    "    return tailProb,eShortfall    \n",
    "              \n",
    "def isMixture(N,M,p,c,l,p1,p2):\n",
    "    theta = np.zeros(M)\n",
    "    cgf = np.zeros(M)\n",
    "    qS = np.zeros([M,N])\n",
    "    S = np.random.gamma(p1, 1/p1, [M]) \n",
    "    wS =  np.transpose(np.tile(1-p2 + p2*S,[N,1]))\n",
    "    pS = np.tile(p,[M,1])*wS\n",
    "    for n in range(0,M):\n",
    "        theta[n] = vc.getSaddlePoint(pS[n,:],c,l,-0.2)\n",
    "        qS[n,:] = getQ(theta[n],c,pS[n,:])\n",
    "        cgf[n] = vc.computeCGF(theta[n],pS[n,:],c)\n",
    "    I = np.transpose(1*np.greater_equal(np.random.poisson(qS,[M,N]),1))\n",
    "    L = np.dot(c,I)\n",
    "    rn = computeRND(theta,L,cgf)        \n",
    "    tailProb = np.mean(np.multiply(L>l,rn)) \n",
    "    eShortfall =  np.mean(np.multiply(L*(L>l),rn))/tailProb        \n",
    "    return tailProb,eShortfall    \n",
    "\n",
    "def computeRND(theta,L,cgf):\n",
    "    return np.exp(-np.multiply(theta,L)+cgf)\n",
    "\n",
    "def isThresholdSimple(N,M,p,c,l,myRho):\n",
    "    mu = getOptimalMeanShift(c,p,l,myRho)\n",
    "    theta = np.zeros(M)\n",
    "    cgf = np.zeros(M)\n",
    "    qZ = np.zeros([M,N])\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    G = np.transpose(np.tile(np.random.normal(mu,1,M),(N,1)))\n",
    "    num = (norm.ppf(p)*np.ones((M,1)))-np.sqrt(myRho)*G\n",
    "    pZ = norm.cdf(np.divide(num,np.sqrt(1-myRho)))\n",
    "    for n in range(0,M):\n",
    "        theta[n] = vc.getSaddlePoint(pZ[n,:],c,l,0.0)\n",
    "        qZ[n,:] = getQ(theta[n],c,pZ[n,:])\n",
    "        cgf[n] = vc.computeCGF(theta[n],pZ[n,:],c)\n",
    "    I = np.transpose(1*np.less(e,norm.ppf(qZ)))\n",
    "    L = np.dot(c,I)\n",
    "    rn = np.exp(-mu*G[:,0]+0.5*(mu**2))*computeRND(theta,L,cgf)\n",
    "    tailProb = np.mean(np.multiply(L>l,rn)) \n",
    "    eShortfall =  np.mean(np.multiply(L*(L>l),rn))/tailProb        \n",
    "    return tailProb,eShortfall    \n",
    "\n",
    "def isThresholdT(N,M,p,c,l,myRho,nu,cm=0):\n",
    "    myShift = (1-2*cm)\n",
    "    mu = getOptimalMeanShift(c,p,l,myRho)\n",
    "    W = np.random.chisquare(nu,M)\n",
    "    myV = W/myShift\n",
    "    theta = np.zeros(M)\n",
    "    cgf = np.zeros(M)\n",
    "    qZ = np.zeros([M,N])\n",
    "    V = np.transpose(np.sqrt(np.tile(myV,(N,1))/nu))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    G = np.transpose(np.tile(np.random.normal(mu,1,M),(N,1)))\n",
    "    num = V*(myT.ppf(p,nu)*np.ones((M,1)))-np.sqrt(myRho)*G\n",
    "    pZ = norm.cdf(np.divide(num,np.sqrt(1-myRho)))\n",
    "    for n in range(0,M):\n",
    "        theta[n] = vc.getSaddlePoint(pZ[n,:],c,l,0.0)\n",
    "        qZ[n,:] = getQ(theta[n],c,pZ[n,:])\n",
    "        cgf[n] = vc.computeCGF(theta[n],pZ[n,:],c)\n",
    "    I = np.transpose(1*np.less(e,norm.ppf(qZ)))\n",
    "    L = np.dot(c,I)\n",
    "    rnChi = np.exp(-cm*myV-(nu/2)*np.log(myShift))\n",
    "    rnMu=np.exp(-mu*G[:,0]+0.5*(mu**2))\n",
    "    rnTwist = computeRND(theta,L,cgf)\n",
    "    rn = rnChi*rnMu*rnTwist\n",
    "    tailProb = np.mean(np.multiply(L>l,rn)) \n",
    "    eShortfall =  np.mean(np.multiply(L*(L>l),rn))/tailProb        \n",
    "    return tailProb,eShortfall    \n",
    "\n",
    "def isThresholdContr(N,M,p,c,l,myRho,nu,cm=0):\n",
    "    myShift = (1-2*cm)\n",
    "    xhat = scipy.optimize.minimize(meanShiftOF,0.01, \n",
    "                                args=(c,p,l,myRho), \n",
    "                                method='SLSQP', jac=None)                             \n",
    "    mu = xhat.x    \n",
    "    theta = np.zeros(M)\n",
    "    cgf = np.zeros(M)\n",
    "    qZ = np.zeros([M,N])\n",
    "    G = np.transpose(np.tile(np.random.normal(mu,1,M),(N,1)))\n",
    "    e = np.random.normal(0,1,[M,N])\n",
    "    W = np.random.chisquare(nu,M)\n",
    "    myV = W/myShift\n",
    "    V = np.transpose(np.sqrt(np.tile(myV,(N,1))/nu))\n",
    "    num = V*myT.ppf(p,nu)*np.ones((M,1))-np.multiply(np.sqrt(myRho),G)\n",
    "    pZ = norm.cdf(np.divide(num,np.sqrt(1-myRho)))\n",
    "    for n in range(0,M):\n",
    "        theta[n] = vc.getSaddlePoint(pZ[n,:],c,l,0.0)\n",
    "        qZ[n,:] = getQ(theta[n],c,pZ[n,:])\n",
    "        cgf[n] = vc.computeCGF(theta[n],pZ[n,:],c)\n",
    "    I = np.transpose(1*np.less(e,norm.ppf(qZ)))\n",
    "    L = np.dot(c,I)         \n",
    "    rnChi=np.exp(-cm*myV-(nu/2)*np.log(myShift))\n",
    "    rnMu=np.exp(-mu*G[:,0]+0.5*mu**2)\n",
    "    rnTwist = computeRND(theta,L,cgf)\n",
    "    rn = rnChi*rnMu*rnTwist\n",
    "    return I,theta,pZ,qZ,cgf,rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff818aa9-c8e4-4bfd-836c-98450d355cb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/cs-training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_29144\\3130314730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Read Training dataset as well as drop the index column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/cs-training.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/cs-training.csv'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read Training dataset as well as drop the index column\n",
    "training_data = pd.read_csv('./data/cs-training.csv').drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "\n",
    "# For each column heading we replace \"-\" and convert the heading in lowercase \n",
    "cleancolumn = []\n",
    "for i in range(len(training_data.columns)):\n",
    "    cleancolumn.append(training_data.columns[i].replace('-', '').lower())\n",
    "training_data.columns = cleancolumn\n",
    "\n",
    "# print the 5 records of the traiing dataset\n",
    "training_data.head()\n",
    "\n",
    "# Describe the all statistical properties of the training dataset\n",
    "training_data[training_data.columns[1:]].describe()\n",
    "\n",
    "training_data[training_data.columns[1:]].median()\n",
    "\n",
    "training_data[training_data.columns[1:]].mean()\n",
    "\n",
    "# This give you the calulation of the target lebels. Which category of the target lebel is how many percentage.\n",
    "total_len = len(training_data['seriousdlqin2yrs'])\n",
    "percentage_labels = (training_data['seriousdlqin2yrs'].value_counts()/total_len)*100\n",
    "percentage_labels\n",
    "\n",
    "# Graphical representation of the target label percentage.\n",
    "sns.set()\n",
    "sns.countplot(training_data.seriousdlqin2yrs).set_title('Data Distribution')\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2.,\n",
    "            height + 2,\n",
    "            '{:.2f}%'.format(100*(height/total_len)),\n",
    "            fontsize=14, ha='center', va='bottom')\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_xlabel(\"Labels for seriousdlqin2yrs attribute\")\n",
    "ax.set_ylabel(\"Numbers of records\")\n",
    "plt.show()\n",
    "\n",
    "# You will get to know which column has missing value and it's give the count that how many records are missing \n",
    "training_data.isnull().sum()\n",
    "\n",
    "# Graphical representation of the missing values.\n",
    "x = training_data.columns\n",
    "y = training_data.isnull().sum()\n",
    "sns.set()\n",
    "sns.barplot(x,y)\n",
    "ax = plt.gca()\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width()/2.,\n",
    "            height + 2,\n",
    "            int(height),\n",
    "            fontsize=14, ha='center', va='bottom')\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_xlabel(\"Data Attributes\")\n",
    "ax.set_ylabel(\"count of missing records for each attribute\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Actual replacement of the missing value using mean value.\n",
    "training_data_mean_replace = training_data.fillna((training_data.mean()))\n",
    "training_data_mean_replace.head()\n",
    "\n",
    "\n",
    "training_data_mean_replace.isnull().sum()\n",
    "\n",
    "# Actual replacement of the missing value using median value.\n",
    "training_data_median_replace = training_data.fillna((training_data.median()))\n",
    "training_data_median_replace.head()\n",
    "\n",
    "training_data_median_replace.isnull().sum()\n",
    "\n",
    "training_data.fillna((training_data.median()), inplace=True)\n",
    "# Get the correlation of the training dataset\n",
    "training_data[training_data.columns[1:]].corr()\n",
    "\n",
    "sns.set()\n",
    "sns.set(font_scale=1.25)\n",
    "sns.heatmap(training_data[training_data.columns[1:]].corr(),annot=True,fmt=\".1f\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Percentile based outlier detection\n",
    "def percentile_based_outlier(data, threshold=95):\n",
    "    diff = (100 - threshold) / 2.0\n",
    "    (minval, maxval) = np.percentile(data, [diff, 100 - diff])\n",
    "    #return minval, maxval\n",
    "    return ((data < minval) | (data > maxval))\n",
    "#percentile_based_outlier(data=training_data.revolvingutilizationofunsecuredlines)\n",
    "\n",
    "# Another percentile based outlier detection method which is based on inter quertile(IQR) range\n",
    "# import numpy as np\n",
    "# def outliers_iqr(ys):\n",
    "#     quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "#     iqr = quartile_3 - quartile_1\n",
    "#     lower_bound = quartile_1 - (iqr * 1.5)\n",
    "#     upper_bound = quartile_3 + (iqr * 1.5)\n",
    "#     return np.where((ys > upper_bound) | (ys < lower_bound))\n",
    "\n",
    "def mad_based_outlier(points, threshold=3.5):\n",
    "    median_y = np.median(points)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in points])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in points]\n",
    "\n",
    "    return np.abs(modified_z_scores) > threshold\n",
    "#mad_based_outlier(points=training_data.age)\n",
    "\n",
    "\n",
    "def std_div(data, threshold=3):\n",
    "    std = data.std()\n",
    "    mean = data.mean()\n",
    "    isOutlier = []\n",
    "    for val in data:\n",
    "        if val/std > threshold:\n",
    "            isOutlier.append(True)\n",
    "        else:\n",
    "            isOutlier.append(False)\n",
    "    return isOutlier\n",
    "#std_div(data=training_data.age)\n",
    "\n",
    "def outlierVote(data):\n",
    "    x = percentile_based_outlier(data)\n",
    "    y = mad_based_outlier(data)\n",
    "    z = std_div(data)\n",
    "    temp = zip(data.index, x, y, z)\n",
    "    final = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i].count(False) >= 2:\n",
    "            final.append(False)\n",
    "        else:\n",
    "            final.append(True)\n",
    "    return final\n",
    "#outlierVote(data=training_data.age)\n",
    "\n",
    "def plotOutlier(x):\n",
    "    fig, axes = plt.subplots(nrows=4)\n",
    "    for ax, func in zip(axes, [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]):\n",
    "        sns.distplot(x, ax=ax, rug=True, hist=False)\n",
    "        outliers = x[func(x)]\n",
    "        ax.plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)\n",
    "\n",
    "    kwargs = dict(y=0.95, x=0.05, ha='left', va='top', size=20)\n",
    "    axes[0].set_title('Percentile-based Outliers', **kwargs)\n",
    "    axes[1].set_title('MAD-based Outliers', **kwargs)\n",
    "    axes[2].set_title('STD-based Outliers', **kwargs)\n",
    "    axes[3].set_title('Majority vote based Outliers', **kwargs)\n",
    "    fig.suptitle('Comparing Outlier Tests with n={}'.format(len(x)), size=20)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15,10)\n",
    "    \n",
    "plotOutlier(training_data.revolvingutilizationofunsecuredlines.sample(5000))\n",
    "plotOutlier(training_data.age.sample(1000))\n",
    "plotOutlier(training_data.numberoftime3059dayspastduenotworse.sample(1000))\n",
    "plotOutlier(training_data.debtratio.sample(1000))\n",
    "plotOutlier(training_data.monthlyincome.sample(1000))\n",
    "plotOutlier(training_data.numberofopencreditlinesandloans.sample(1000))\n",
    "plotOutlier(training_data.numberoftimes90dayslate.sample(1000))\n",
    "plotOutlier(training_data.numberrealestateloansorlines.sample(1000))\n",
    "plotOutlier(training_data.numberoftime6089dayspastduenotworse.sample(1000))\n",
    "plotOutlier(training_data.numberofdependents.sample(1000))\n",
    "\n",
    "revNew = []\n",
    "training_data.revolvingutilizationofunsecuredlines\n",
    "for val in training_data.revolvingutilizationofunsecuredlines:\n",
    "    if val <= 0.99999:\n",
    "        revNew.append(val)\n",
    "    else:\n",
    "        revNew.append(0.99999)\n",
    "training_data.revolvingutilizationofunsecuredlines = revNew\n",
    "training_data.age.plot.box()\n",
    "\n",
    "import collections\n",
    "collections.Counter(training_data.age)\n",
    "\n",
    "ageNew = []\n",
    "for val in training_data.age:\n",
    "    if val > 21:\n",
    "        ageNew.append(val)\n",
    "    else:\n",
    "        ageNew.append(21)\n",
    "        \n",
    "training_data.age = ageNew\n",
    "\n",
    "collections.Counter(training_data.numberoftime3059dayspastduenotworse)\n",
    "\n",
    "New = []\n",
    "med = training_data.numberoftime3059dayspastduenotworse.median()\n",
    "for val in training_data.numberoftime3059dayspastduenotworse:\n",
    "    if ((val == 98) | (val == 96)):\n",
    "        New.append(med)\n",
    "    else:\n",
    "        New.append(val)\n",
    "\n",
    "training_data.numberoftime3059dayspastduenotworse = New\n",
    "\n",
    "def outlierRatio(data):\n",
    "    functions = [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]\n",
    "    outlierDict = {}\n",
    "    for func in functions:\n",
    "        funcResult = func(data)\n",
    "        count = 0\n",
    "        for val in funcResult:\n",
    "            if val == True:\n",
    "                count += 1 \n",
    "        outlierDict[str(func)[10:].split()[0]] = [count, '{:.2f}%'.format((float(count)/len(data))*100)]\n",
    "    \n",
    "    return outlierDict\n",
    "outlierRatio(training_data.debtratio)\n",
    "\n",
    "plotOutlier(training_data.debtratio.sample(1000))\n",
    "\n",
    "\n",
    "def add_freq():\n",
    "    ncount = len(training_data)\n",
    "\n",
    "    ax2=ax.twinx()\n",
    "\n",
    "    ax2.yaxis.tick_left()\n",
    "    ax.yaxis.tick_right()\n",
    "\n",
    "    ax.yaxis.set_label_position('right')\n",
    "    ax2.yaxis.set_label_position('left')\n",
    "\n",
    "    ax2.set_ylabel('Frequency [%]')\n",
    "\n",
    "    for p in ax.patches:\n",
    "        x=p.get_bbox().get_points()[:,0]\n",
    "        y=p.get_bbox().get_points()[1,1]\n",
    "        ax.annotate('{:.1f}%'.format(100.*y/ncount), (x.mean(), y), \n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    ax2.set_ylim(0,100)\n",
    "    ax2.grid(None)\n",
    "ax = sns.countplot(mad_based_outlier(training_data.debtratio))\n",
    "\n",
    "add_freq()\n",
    "\n",
    "minUpperBound = min([val for (val, out) in zip(training_data.debtratio, mad_based_outlier(training_data.debtratio)) if out == True])\n",
    "\n",
    "newDebtRatio = []\n",
    "for val in training_data.debtratio:\n",
    "    if val > minUpperBound:\n",
    "        newDebtRatio.append(minUpperBound)\n",
    "    else:\n",
    "        newDebtRatio.append(val)\n",
    "\n",
    "training_data.debtratio = newDebtRatio \n",
    "\n",
    "def plotOutlierFree(x):\n",
    "    fig, axes = plt.subplots(nrows=4)\n",
    "    nOutliers = []\n",
    "    for ax, func in zip(axes, [percentile_based_outlier, mad_based_outlier, std_div, outlierVote]):\n",
    "        tfOutlier = zip(x, func(x))\n",
    "        nOutliers.append(len([index for (index, bol) in tfOutlier if bol == True]))\n",
    "        outlierFree = [index for (index, bol) in tfOutlier if bol == True]\n",
    "        sns.distplot(outlierFree, ax=ax, rug=True, hist=False)\n",
    "        \n",
    "    kwargs = dict(y=0.95, x=0.05, ha='left', va='top', size=15)\n",
    "    axes[0].set_title('Percentile-based Outliers, removed: {r}'.format(r=nOutliers[0]), **kwargs)\n",
    "    axes[1].set_title('MAD-based Outliers, removed: {r}'.format(r=nOutliers[1]), **kwargs)\n",
    "    axes[2].set_title('STD-based Outliers, removed: {r}'.format(r=nOutliers[2]), **kwargs)\n",
    "    axes[3].set_title('Majority vote based Outliers, removed: {r}'.format(r=nOutliers[3]), **kwargs)\n",
    "    fig.suptitle('Outlier Removed By Method with n={}'.format(len(x)), size=20)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(15,10)\n",
    "    \n",
    "plotOutlierFree(training_data.monthlyincome.sample(1000)) \n",
    "    \n",
    "def replaceOutlier(data, method = outlierVote, replace='median'):\n",
    "    '''replace: median (auto)\n",
    "                'minUpper' which is the upper bound of the outlier detection'''\n",
    "    vote = outlierVote(data)\n",
    "    x = pd.DataFrame(zip(data, vote), columns=['debt', 'outlier'])\n",
    "    if replace == 'median':\n",
    "        replace = x.debt.median()\n",
    "    elif replace == 'minUpper':\n",
    "        replace = min([val for (val, vote) in zip(data, vote) if vote == True])\n",
    "        if replace < data.mean():\n",
    "            return 'There are outliers lower than the sample mean'\n",
    "    debtNew = []\n",
    "    for i in range(x.shape[0]):\n",
    "        if x.iloc[i][1] == True:\n",
    "            debtNew.append(replace)\n",
    "        else:\n",
    "            debtNew.append(x.iloc[i][0])\n",
    "    \n",
    "    return debtNew\n",
    "    \n",
    "    \n",
    "incomeNew = replaceOutlier(training_data.monthlyincome, replace='minUpper')\n",
    "\n",
    "training_data.monthlyincome = incomeNew\n",
    "\n",
    "collections.Counter(training_data.numberoftimes90dayslate)\n",
    "\n",
    "def removeSpecificAndPutMedian(data, first = 98, second = 96):\n",
    "    New = []\n",
    "    med = data.median()\n",
    "    for val in data:\n",
    "        if ((val == first) | (val == second)):\n",
    "            New.append(med)\n",
    "        else:\n",
    "            New.append(val)\n",
    "            \n",
    "    return New\n",
    "\n",
    "new = removeSpecificAndPutMedian(training_data.numberoftimes90dayslate)\n",
    "\n",
    "training_data.numberoftimes90dayslate = new\n",
    "\n",
    "collections.Counter(training_data.numberrealestateloansorlines)\n",
    "\n",
    "realNew = []\n",
    "for val in training_data.numberrealestateloansorlines:\n",
    "    if val > 17:\n",
    "        realNew.append(17)\n",
    "    else:\n",
    "        realNew.append(val) \n",
    "training_data.numberrealestateloansorlines = realNew\n",
    "\n",
    "collections.Counter(training_data.numberoftime6089dayspastduenotworse)\n",
    "\n",
    "new = removeSpecificAndPutMedian(training_data.numberoftime6089dayspastduenotworse)\n",
    "training_data.numberoftime6089dayspastduenotworse = new\n",
    "\n",
    "collections.Counter(training_data.numberofdependents)\n",
    "\n",
    "depNew = []\n",
    "for var in training_data.numberofdependents:\n",
    "    if var > 10:\n",
    "        depNew.append(10)\n",
    "    else:\n",
    "        depNew.append(var)\n",
    "\n",
    "training_data.numberofdependents = depNew\n",
    "\n",
    "training_data.head()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "training_data.columns[1:]\n",
    "\n",
    "X = training_data.drop('seriousdlqin2yrs', axis=1)\n",
    "y = training_data.seriousdlqin2yrs\n",
    "features_label = training_data.columns[1:]\n",
    "forest = RandomForestClassifier (n_estimators = 10000, random_state=0, n_jobs = -1)\n",
    "forest.fit(X,y)\n",
    "importances = forest.feature_importances_\n",
    "indices = np. argsort(importances)[::-1]\n",
    "for i in range(X.shape[1]):\n",
    "    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]),features_label, rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X = training_data.drop('seriousdlqin2yrs', axis=1)\n",
    "y = training_data.seriousdlqin2yrs\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "knMod = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None)\n",
    "\n",
    "knMod.fit(X_train, y_train)\n",
    "\n",
    "knMod.score(X_test, y_test)\n",
    "\n",
    "test_labels=knMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "glmMod = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight=None, \n",
    "                            random_state=None, solver='liblinear', max_iter=100,\n",
    "                            multi_class='ovr', verbose=2)\n",
    "\n",
    "\n",
    "glmMod.fit(X_train, y_train)\n",
    "\n",
    "glmMod.score(X_test, y_test)\n",
    "\n",
    "test_labels=glmMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "adaMod = AdaBoostClassifier(base_estimator=None, n_estimators=200, learning_rate=1.0)\n",
    "\n",
    "adaMod.fit(X_train, y_train)\n",
    "\n",
    "adaMod.score(X_test, y_test)\n",
    "\n",
    "test_labels=adaMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "gbMod = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,\n",
    "                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                   max_depth=3,\n",
    "                                   init=None, random_state=None, max_features=None, verbose=0)\n",
    "\n",
    "gbMod.fit(X_train, y_train)\n",
    "\n",
    "gbMod.score(X_test, y_test)\n",
    "\n",
    "test_labels=gbMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "rfMod = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                               max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, \n",
    "                               random_state=None, verbose=0)\n",
    "rfMod.fit(X_train, y_train)\n",
    "\n",
    "rfMod.score(X_test, y_test)\n",
    "\n",
    "test_labels=rfMod.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def cvDictGen(functions, scr, X_train=X, y_train=y, cv=5, verbose=1):\n",
    "    cvDict = {}\n",
    "    for func in functions:\n",
    "        cvScore = cross_val_score(func, X_train, y_train, cv=cv, verbose=verbose, scoring=scr)\n",
    "        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n",
    "    \n",
    "    return cvDict\n",
    "\n",
    "def cvDictNormalize(cvDict):\n",
    "    cvDictNormalized = {}\n",
    "    for key in cvDict.keys():\n",
    "        for i in cvDict[key]:\n",
    "            cvDictNormalized[key] = ['{:0.2f}'.format((cvDict[key][0]/cvDict[cvDict.keys()[0]][0])),\n",
    "                                     '{:0.2f}'.format((cvDict[key][1]/cvDict[cvDict.keys()[0]][1]))]\n",
    "    return cvDictNormalized\n",
    "\n",
    "cvD = cvDictGen(functions=[knMod, glmMod, adaMod, gbMod, rfMod], scr='roc_auc')\n",
    "cvD\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "adaHyperParams = {'n_estimators': [10,50,100,200,420]}\n",
    "\n",
    "gridSearchAda = RandomizedSearchCV(estimator=adaMod, param_distributions=adaHyperParams, n_iter=5,\n",
    "                                   scoring='roc_auc', fit_params=None, cv=None, verbose=2).fit(X_train, y_train)\n",
    "\n",
    "gridSearchAda.best_params_, gridSearchAda.best_score_\n",
    "\n",
    "gbHyperParams = {'loss' : ['deviance', 'exponential'],\n",
    "                 'n_estimators': randint(10, 500),\n",
    "                 'max_depth': randint(1,10)}\n",
    "\n",
    "gridSearchGB = RandomizedSearchCV(estimator=gbMod, param_distributions=gbHyperParams, n_iter=10,\n",
    "                                   scoring='roc_auc', fit_params=None, cv=None, verbose=2).fit(X_train, y_train)\n",
    "\n",
    "gridSearchGB.best_params_, gridSearchGB.best_score_\n",
    "\n",
    "\n",
    "bestGbModFitted = gridSearchGB.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "bestAdaModFitted = gridSearchAda.best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "cvDictbestpara = cvDictGen(functions=[bestGbModFitted, bestAdaModFitted], scr='roc_auc')\n",
    "\n",
    "cvDictbestpara\n",
    "\n",
    "test_labels=bestGbModFitted.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "test_labels=bestAdaModFitted.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X_train_1 = np.array(X_train)\n",
    "X_train_transform = transformer.transform(X_train_1)\n",
    "\n",
    "bestGbModFitted_transformed = gridSearchGB.best_estimator_.fit(X_train_transform, y_train)\n",
    "\n",
    "bestAdaModFitted_transformed = gridSearchAda.best_estimator_.fit(X_train_transform, y_train)\n",
    "\n",
    "cvDictbestpara_transform = cvDictGen(functions=[bestGbModFitted_transformed, bestAdaModFitted_transformed],\n",
    "                                     scr='roc_auc')\n",
    "\n",
    "cvDictbestpara_transform\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X_test_1 = np.array(X_test)\n",
    "X_test_transform = transformer.transform(X_test_1)\n",
    "\n",
    "X_test_transform\n",
    "\n",
    "test_labels=bestGbModFitted_transformed.predict_proba(np.array(X_test_transform))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "test_labels=bestAdaModFitted_transformed.predict_proba(np.array(X_test_transform))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "votingMod = VotingClassifier(estimators=[('gb', bestGbModFitted_transformed), \n",
    "                                         ('ada', bestAdaModFitted_transformed)], voting='soft',weights=[2,1])\n",
    "votingMod = votingMod.fit(X_train_transform, y_train)\n",
    "\n",
    "\n",
    "test_labels=votingMod.predict_proba(np.array(X_test_transform))[:,1]\n",
    "\n",
    "votingMod.score(X_test_transform, y_test)\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "votingMod_old = VotingClassifier(estimators=[('gb', bestGbModFitted), ('ada', bestAdaModFitted)], \n",
    "                                 voting='soft',weights=[2,1])\n",
    "votingMod_old = votingMod.fit(X_train, y_train)\n",
    "\n",
    "test_labels=votingMod_old.predict_proba(np.array(X_test.values))[:,1]\n",
    "\n",
    "roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af2eda8e-0414-4318-b2ab-2d4cc2dd2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "msft = yf.Ticker(\"AAPL\")\n",
    "#msft.info\n",
    "msft.history_metadata\n",
    "\n",
    "stock = yf.Ticker(ticker)                           \n",
    "temp = pd.DataFrame(stock.dividends)\n",
    "div = temp.loc[start:end]\n",
    "\n",
    "msft.get_shares_full(start=\"2022-01-01\", end=None)\n",
    "\n",
    "msft.income_stmt\n",
    "msft.quarterly_income_stmt\n",
    "\n",
    "msft.balance_sheet\n",
    "msft.quarterly_balance_sheet\n",
    "\n",
    "msft.cashflow\n",
    "msft.quarterly_cashflow\n",
    "\n",
    "# show holders\n",
    "msft.major_holders\n",
    "msft.institutional_holders\n",
    "msft.mutualfund_holders\n",
    "\n",
    "# show news\n",
    "msft.news\n",
    "\n",
    "msft.options #option dates\n",
    "opt = msft.option_chain('2025-12-19')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de17148f-7f46-4d7d-b778-97622d6fc9a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58128cab-c04e-40d5-a2db-8da987bdfefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
